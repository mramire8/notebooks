{
 "metadata": {
  "name": "",
  "signature": "sha256:5d94a7ee89e30add0590c48908e0da63dc39756ba65f9a868299c30b2bfc7bc8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Structured Reading: User Studies\n",
      "\n",
      "## This Notebook\n",
      "\n",
      "The objective is to prepare the users studies data and test some configurations\n",
      "\n",
      "## Test Configuration\n",
      "\n",
      "* Set experiment\n",
      "* Load data\n",
      "* Select documents\n",
      "* Select snippets\n",
      "* Save data in a file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Imports \n",
      "%matplotlib inline\n",
      "\n",
      "STRUCTURED = '/Users/maru/MyCode/structured'\n",
      "IMDB_DATA='/Users/maru/MyCode/data/imdb'\n",
      "SRAA_DATA='/Users/maru/MyCode/data/imdb'\n",
      "TWIITER_DATA = ''\n",
      "\n",
      "# STRUCTURED = '/Users/maru/My Code/structured'\n",
      "# IMDB_DATA='/Users/maru/Dataset/aclImdb'\n",
      "# SRAA_DATA='/Users/maru/Dataset/aviation/data'\n",
      "# TWIITER_DATA = '/Users/maru/Dataset/twitter'\n",
      "\n",
      "import sys\n",
      "import os\n",
      "sys.path.append(os.path.abspath(STRUCTURED))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import utilities.experimentutils as exputil\n",
      "import utilities.datautils as datautil\n",
      "import numpy as np\n",
      "import nltk\n",
      "from sklearn import metrics\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib as mpl\n",
      "\n",
      "mpl.style.use('bmh')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loading Data\n",
      "vct = exputil.get_vectorizer({'vectorizer':\"tfidf\", 'limit':None, 'min_size':None})\n",
      "# Sentence tokenizers\n",
      "sent_tk = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "def load_data(dataname, path):\n",
      "    import pickle\n",
      "\n",
      "    DATA_PKL = path + '/data.pkl'\n",
      "\n",
      "    if os.path.isfile(DATA_PKL):\n",
      "        vct, data = pickle.load(open(DATA_PKL, 'rb'))\n",
      "    else:\n",
      "        vct = exputil.get_vectorizer({'vectorizer':\"tfidf\", 'limit':None, 'min_size':None})\n",
      "        data = datautil.load_dataset(dataname, path, categories=None, rnd=5463, shuffle=True)\n",
      "        data.train.data = np.array(data.train.data, dtype=object)\n",
      "        data.test.data = np.array(data.test.data, dtype=object)\n",
      "        data.train.bow = vct.fit_transform(data.train.data)\n",
      "        data.test.bow = vct.transform(data.test.data)\n",
      "        pickle.dump((vct, data), open(DATA_PKL, 'wb'))\n",
      "\n",
      "    return data, vct\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the sentences for testing\n",
      "def _sentences(docs, doc_labels, sent_tk):\n",
      "    data = []\n",
      "    true_labels = []\n",
      "    sent = sent_tk.tokenize_sents(docs)\n",
      "    for sentences, doc_label in zip(sent, doc_labels):\n",
      "        data.extend(sentences)\n",
      "        true_labels.extend([doc_label] * len(sentences))\n",
      "    return data, np.array(true_labels)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# select random snippets\n",
      "def select_random_snippets(data, rnd, sent_tk, n=1000):\n",
      "    rnd_docs = rnd.permutation(len(data.target))\n",
      "    docs_text = data.data[rnd_docs[:n]]\n",
      "    docs_lbl  = data.target[rnd_docs[:n]]\n",
      "    docs_sent = sent_tk.tokenize_sents(docs_text)\n",
      "    # docs_sents = _sentences(cost_text, docs_lbl, sent_tk)\n",
      "    rnd_st = np.random.RandomState(543210)\n",
      "    selected = []\n",
      "    for lbl, sents in zip(docs_lbl, docs_sent):\n",
      "        if len(sents) > 1:\n",
      "            picked = np.random.random_integers(len(sents)-1)\n",
      "        else:\n",
      "            picked = 0\n",
      "        selected.append(sents[picked])\n",
      "    return docs_text, docs_lbl, selected    \n",
      "\n",
      "# print snippets into a file\n",
      "def output_test(name, doc, lbl, snip):\n",
      "    file_name = \"./output/rnd_\" + name + \".txt\"\n",
      "    if not os.path.exists(\"./output/\"):\n",
      "        os.makedirs(\"./output\")\n",
      "    f = open(file_name, \"w\")\n",
      "    f.write(\"LABEL\\tSENT\\tDOC\\n\")\n",
      "    for a, b, c in zip(lbl, snip, doc):\n",
      "#         print \"{}\\t{}\\t{}\\n\".format(a, b, c)\n",
      "        f.write(\"{}\\t{}\\t{}\\n\".format(a, b.encode('utf-8'), c.encode('utf-8').replace(\"\\n\",\". \")))\n",
      "    f.close()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the dataset\n",
      "imdb, vct = load_data('imdb', IMDB_DATA)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get testing files for IMDB\n",
      "rnd = np.random.RandomState(2345)\n",
      "# Get the sampled snippets\n",
      "docs, lbl, snip = select_random_snippets(imdb.train, rnd, sent_tk, n=1000)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output_test(\"imdb\", docs,lbl, snip)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"this is a sentence.\\n This is the other sentence.\".replace(\"\\n\", \". \")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "'this is a sentence..  This is the other sentence.'"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the dataset\n",
      "sraa, vct = load_data('sraa', SRAA_DATA)\n",
      "docs, lbl, snip = select_random_snippets(sraa.train, rnd, sent_tk, n=1000)\n",
      "output_test(\"sraa\", docs,lbl, snip)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the dataset\n",
      "twitter, vct = load_data('twitter', SRAA_DATA)\n",
      "tw_tk = exputil.get_tokenizer('tweets')\n",
      "docs, lbl, snip = select_random_snippets(twitter.train, rnd, tw_tk, n=1000)\n",
      "output_test(\"twitter\", docs,lbl, snip)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}