{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Budget Analysis for SR methods\n",
    "\n",
    "The analysis has the following parts: \n",
    "\n",
    "1. Maximum accuracy obtained per noise level\n",
    "1. Basic learning curve \n",
    "1. Cost of annotation of the learning curve \n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Compute maximum accuracy for dataset\n",
    "1. Compute maximum accuracy for fist-1\n",
    "1. Compute levels of performance\n",
    "1. Compute budget locations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.636\n"
     ]
    }
   ],
   "source": [
    "DATA = 'C:/Users/mramire8/Documents/Research/Oracle confidence and Interruption/dataset/aclImdb/raw-data'\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('C:/cygwin/home/mramire8/python_code/structured/'))\n",
    "sys.path.append(os.path.abspath('/Users/maru/MyCode/structured/'))\n",
    "\n",
    "import learner\n",
    "\n",
    "from utilities.datautils import load_dataset\n",
    "import utilities.experimentutils as exputil\n",
    "import numpy as np\n",
    "import experiment.base as exp\n",
    "import nltk\n",
    "\n",
    "\n",
    "# EXample of IMDB \n",
    "kvalues = [10, 25, 50, 75, 100]\n",
    "kv = [ 25, 50, 75, 100]\n",
    "cost = np.array([5.7, 8.2, 10.9, 15.9, 16.7])\n",
    "\n",
    "def fn_int(x, cost, kvalues):\n",
    "    \n",
    "    binx = min(np.digitize([x], kvalues)[0], len(cost)-1)\n",
    "    lbbinx = max(binx-1, 0) \n",
    "#     print binx, lbbinx\n",
    "    y1 = cost[lbbinx] if lbbinx>=0  else 0\n",
    "    y2 = cost[binx]\n",
    "    x1 = kvalues[lbbinx] if lbbinx >=0 else 0\n",
    "    x2 = kvalues[binx]\n",
    "#     print x1,x2,y1,y2\n",
    "    \n",
    "    m = (y2-y1) / (x2-x1)\n",
    "    b = y2 - m * x2\n",
    "    \n",
    "    if x < kvalues[0]:\n",
    "        y = cost[0]\n",
    "    elif x > kvalues[-1] :\n",
    "        y = cost[-1]\n",
    "    else:\n",
    "        y = (m * x) + b\n",
    "    return y\n",
    "\n",
    "print fn_int(98, cost, kvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get the data ready\n",
    "imdb_path = 'C:/Users/mramire8/Documents/Research/Oracle confidence and Interruption/dataset/aclImdb/raw-data'\n",
    "# imdb_path = '/Users/maru/MyCode/data/imdb'\n",
    "\n",
    "vct = exputil.get_vectorizer({'vectorizer':'bow', 'limit':None, 'min_size':2})\n",
    "\n",
    "imdb =  load_dataset(\"imdb\",imdb_path, keep_subject=True)\n",
    "\n",
    "# sraa = load_dataset(\"aviation\", 100, categories[0], vct2, 100, raw=True,  percent=.5, keep_subject=True)\n",
    "imdb.train.bow = vct.fit_transform(imdb.train.data)\n",
    "imdb.test.bow = vct.transform(imdb.test.data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6b980925b4b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     12\u001b[0m \u001b[0mimdb_expert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexputil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpert_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mimdb_expert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvct\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\cygwin\\home\\mramire8\\python_code\\structured\\expert\\noisy_expert.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_text, y, vct)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvct\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0msx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 236\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\scipy\\sparse\\base.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "imdb.test.bow = vct.transform(imdb.test.data)\n",
    "\n",
    "expert_config = {'type'           : \"noisyreluctantscale\", \n",
    "    'model'          : \"lrl1\",\n",
    "    'parameter'      : 0.3,\n",
    "    'costmodel'      : 1,\n",
    "    'sent_tokenizer' : \"first1snippet\",\n",
    "    'snip_size'      : (1,1),\n",
    "    'threshold'      : .4,\n",
    "    'scale': 0.\n",
    "}\n",
    "imdb_expert = exputil.get_expert(expert_config, size=len(imdb.train.data))\n",
    "imdb_expert.fit(imdb.train.bow, y=imdb.train.target, vct=vct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for every document, get the snippet, convert to bow, \n",
    "# get the label from expert\n",
    "# train a student classifier on resulting dataset\n",
    "# test levels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "# test student "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the data for the labels \n",
    "def add_instance(mat, to_add):\n",
    "    test_sent = mat\n",
    "    if len(to_add) == 0: #.shape[0] == 0:\n",
    "        return mat\n",
    "    \n",
    "    for instance in to_add:\n",
    "        if isinstance(test_sent, list):\n",
    "            test_sent = instance\n",
    "        else:\n",
    "            test_sent = vstack([test_sent, instance], format='csr')\n",
    "    return test_sent\n",
    "\n",
    "def convert2mat(tx_lst, target):\n",
    "    tx = []\n",
    "    ty = []\n",
    "    for x,y in zip(txt_lst, target):\n",
    "        tx.extend(x)\n",
    "        ty.extend([y]*len(x))\n",
    "    return np.array(tx), np.array(ty)\n",
    "\n",
    "def create_data(org_data, expert,  snip_tk, vct):\n",
    "    snips = snip_tk.tokenize_sents(org_data.data)\n",
    "    tx, y = convert2mat(snips, org_data.target)\n",
    "    vecx = vct.transform(tx)\n",
    "    pred = expert.label({'bow':vecx,'snippet':tx, 'index':range(len(tx))}, y=y)\n",
    "    return tx, vecx, pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "def test_level(org_data, expert, vct, clf)\n",
    "    rnd_state = np.random.RandomState(123)\n",
    "    train_sizes=np.linspace(.1, 1.0, 5)\n",
    "\n",
    "    tk = exputil.get_tokenizer('first1snippet',snip_size=(s,s))\n",
    "    cv = cross_validation.ShuffleSplit(n, n_iter=5, test_size=0.0,\n",
    "                                       random_state=rnd_state)\n",
    "    tx, bow, pred = create_data(org_data.train, expert, tk, vct)\n",
    "    sizes = np.array([len(x.split()) for x in tx])\n",
    "    cost = np.array([fn_int(x, cost, kvalues) for x in sizes])\n",
    "    \n",
    "    results = []\n",
    "    for train_index, _ in cv:\n",
    "        \n",
    "test_level(imdb, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
