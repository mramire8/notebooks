{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Budget Analysis for SR methods\n",
    "\n",
    "The analysis has the following parts: \n",
    "\n",
    "1. Maximum accuracy obtained per noise level\n",
    "1. Basic learning curve \n",
    "1. Cost of annotation of the learning curve \n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Compute maximum accuracy for dataset\n",
    "1. Compute maximum accuracy for fist-1\n",
    "1. Compute levels of performance\n",
    "1. Compute budget locations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.636\n"
     ]
    }
   ],
   "source": [
    "DATA = 'C:/Users/mramire8/Documents/Research/Oracle confidence and Interruption/dataset/aclImdb/raw-data'\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('C:/cygwin/home/mramire8/python_code/structured/'))\n",
    "sys.path.append(os.path.abspath('/Users/maru/MyCode/structured/'))\n",
    "\n",
    "import learner\n",
    "\n",
    "from utilities.datautils import load_dataset\n",
    "import utilities.experimentutils as exputil\n",
    "import numpy as np\n",
    "import experiment.base as exp\n",
    "import nltk\n",
    "\n",
    "\n",
    "# EXample of IMDB \n",
    "kvalues = [10, 25, 50, 75, 100]\n",
    "kv = [ 25, 50, 75, 100]\n",
    "cost = np.array([5.7, 8.2, 10.9, 15.9, 16.7])\n",
    "\n",
    "def fn_int(x, cost, kvalues):\n",
    "    \n",
    "    binx = min(np.digitize([x], kvalues)[0], len(cost)-1)\n",
    "    lbbinx = max(binx-1, 0) \n",
    "#     print binx, lbbinx\n",
    "    y1 = cost[lbbinx] if lbbinx>=0  else 0\n",
    "    y2 = cost[binx]\n",
    "    x1 = kvalues[lbbinx] if lbbinx >=0 else 0\n",
    "    x2 = kvalues[binx]\n",
    "#     print x1,x2,y1,y2\n",
    "    \n",
    "    m = (y2-y1) / (x2-x1)\n",
    "    b = y2 - m * x2\n",
    "    \n",
    "    if x < kvalues[0]:\n",
    "        y = cost[0]\n",
    "    elif x > kvalues[-1] :\n",
    "        y = cost[-1]\n",
    "    else:\n",
    "        y = (m * x) + b\n",
    "    return y\n",
    "\n",
    "print fn_int(98, cost, kvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get the data ready\n",
    "imdb_path = 'C:/Users/mramire8/Documents/Research/Oracle confidence and Interruption/dataset/aclImdb/raw-data'\n",
    "imdb_path = '/Users/maru/MyCode/data/imdb'\n",
    "\n",
    "vct = exputil.get_vectorizer({'vectorizer':'bow', 'limit':None, 'min_size':2})\n",
    "\n",
    "imdb =  load_dataset(\"imdb\",imdb_path, keep_subject=True)\n",
    "\n",
    "# sraa = load_dataset(\"aviation\", 100, categories[0], vct2, 100, raw=True,  percent=.5, keep_subject=True)\n",
    "imdb.train.bow = vct.fit_transform(imdb.train.data)\n",
    "imdb.test.bow = vct.transform(imdb.test.data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<expert.noisy_expert.NoisyReluctantDocumentExpert at 0x11f0c9510>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imdb.test.bow = vct.transform(imdb.test.data)\n",
    "\n",
    "expert_config = {'type'           : \"noisyreluctantscale\", \n",
    "    'model'          : \"lrl1\",\n",
    "    'parameter'      : 0.3,\n",
    "    'costmodel'      : 1,\n",
    "    'sent_tokenizer' : \"first1snippet\",\n",
    "    'snip_size'      : (1,1),\n",
    "    'threshold'      : .4,\n",
    "    'scale': 0.\n",
    "}\n",
    "imdb_expert = exputil.get_expert(expert_config, size=len(imdb.train.data))\n",
    "imdb_expert.fit(imdb.train.data, y=imdb.train.target, vct=vct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24991, 267154)\n"
     ]
    }
   ],
   "source": [
    "print imdb.train.bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for every document, get the snippet, convert to bow, \n",
    "# get the label from expert\n",
    "# train a student classifier on resulting dataset\n",
    "# test levels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "# test student "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the data for the labels \n",
    "def add_instance(mat, to_add):\n",
    "    test_sent = mat\n",
    "    if len(to_add) == 0: #.shape[0] == 0:\n",
    "        return mat\n",
    "    \n",
    "    for instance in to_add:\n",
    "        if isinstance(test_sent, list):\n",
    "            test_sent = instance\n",
    "        else:\n",
    "            test_sent = vstack([test_sent, instance], format='csr')\n",
    "    return test_sent\n",
    "\n",
    "def convert2mat(txt_lst, target):\n",
    "    tx = []\n",
    "    ty = []\n",
    "    for x,y in zip(txt_lst, target):\n",
    "        tx.extend(x)\n",
    "        ty.extend([y]*len(x))\n",
    "    return np.array(tx), np.array(ty)\n",
    "\n",
    "from sklearn.datasets import base as bunch \n",
    "\n",
    "def create_data(org_data, expert,  snip_tk, vct):\n",
    "    snips = snip_tk.tokenize_sents(org_data.data)\n",
    "    tx, y = convert2mat(snips, org_data.target)\n",
    "    vecx = vct.transform(tx)\n",
    "    query = bunch.Bunch(index=range(len(tx)), snippet=tx, bow=vecx)\n",
    "#     query = {'bow':vecx,'snippet':tx, 'index':range(len(tx))}\n",
    "    pred = expert.label(query, y=y)\n",
    "    \n",
    "    non_neu = np.array([i for i,x in enumerate(pred) if x is not None])\n",
    "    return tx[non_neu], vecx[non_neu], np.array(pred[non_neu], dtype=int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "Error scale: 0.0\n",
      "Average size: 22.6506931696\n",
      "Average cost: 7.56268067777\n",
      "Accuracy w/o neutrals: 0.837208371684\n",
      "===============\n",
      "Error scale: 0.142\n",
      "Average size: 22.6506931696\n",
      "Average cost: 7.56268067777\n",
      "Accuracy w/o neutrals: 0.830965624875\n",
      "===============\n",
      "Error scale: 0.284\n",
      "Average size: 22.6506931696\n",
      "Average cost: 7.56268067777\n",
      "Accuracy w/o neutrals: 0.819280483413\n",
      "===============\n",
      "Error scale: 0.425\n",
      "Average size: 22.6506931696\n",
      "Average cost: 7.56268067777\n",
      "Accuracy w/o neutrals: 0.806754972188\n",
      "===============\n",
      "Error scale: 0.567\n",
      "Average size: 22.6506931696\n",
      "Average cost: 7.56268067777\n",
      "Accuracy w/o neutrals: 0.798351274561\n",
      "===============\n",
      "Error scale: 0.709\n",
      "Average size: 22.6506931696\n",
      "Average cost: 7.56268067777\n",
      "Accuracy w/o neutrals: 0.787986714154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/kernel/__main__.py:32: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import metrics \n",
    "\n",
    "def test_level(org_data, expert, vct, s, cost, kvalues, errors):\n",
    "    ''' Test maximum performance of an student when using data from first-1 and 0 noise.\n",
    "    '''\n",
    "    rnd_state = np.random.RandomState(123)\n",
    "#     train_sizes=np.linspace(.1, 1.0, 5)\n",
    "    tk = exputil.get_tokenizer('first1snippet',snip_size=(s,s))\n",
    "\n",
    "#     cv = cross_validation.ShuffleSplit(n, n_iter=5, test_size=0.0,\n",
    "#                                        random_state=rnd_state)\n",
    "    results = []\n",
    "\n",
    "    for err in errors:\n",
    "        print \"=\" * 15\n",
    "        print \"Error scale: %s\" % err\n",
    "        expert.set_scale_factor(err)\n",
    "        tx, bow, y = create_data(org_data.train, expert, tk, vct)\n",
    "\n",
    "        sizes = np.array([len(x.split()) for x in tx])\n",
    "        cost2 = np.array([fn_int(x, cost, kvalues) for x in sizes])\n",
    "        print \"Average size: %s\" % np.mean(sizes)\n",
    "        print \"Average cost: %s\" % np.mean(cost2)\n",
    "        clf = exputil.get_classifier('lrl1', parameter=1.)\n",
    "        clf.fit(bow, y)\n",
    "        pred = clf.predict(org_data.test.bow)\n",
    "        accu = metrics.accuracy_score(org_data.test.target, pred)\n",
    "        print \"Accuracy w/o neutrals: %s\" % accu\n",
    "errors = [0.,0.142,0.284,0.425,0.567,0.709] # scale for IMDB\n",
    "test_level(imdb, imdb_expert, vct, 1, cost, kvalues, errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for SRAA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "Error scale: 0.0\n",
      "Average size: 22.8930652028\n",
      "Average cost: 6.14693315408\n",
      "Accuracy w/o neutrals: 0.926454115783\n",
      "===============\n",
      "Error scale: 0.138\n",
      "Average size: 22.8930652028\n",
      "Average cost: 6.14693315408\n",
      "Accuracy w/o neutrals: 0.911154823375\n",
      "===============\n",
      "Error scale: 0.276\n",
      "Average size: 22.8930652028\n",
      "Average cost: 6.14693315408\n",
      "Accuracy w/o neutrals: 0.907548561593\n",
      "===============\n",
      "Error scale: 0.414\n",
      "Average size: 22.8930652028\n",
      "Average cost: 6.14693315408\n",
      "Accuracy w/o neutrals: 0.907739802748\n",
      "===============\n",
      "Error scale: 0.552\n",
      "Average size: 22.8930652028\n",
      "Average cost: 6.14693315408\n",
      "Accuracy w/o neutrals: 0.90088244133\n",
      "===============\n",
      "Error scale: 0.69\n",
      "Average size: 22.8930652028\n",
      "Average cost: 6.14693315408\n",
      "Accuracy w/o neutrals: 0.894598803377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/IPython/kernel/__main__.py:32: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "## Get the data ready\n",
    "sraa_path = 'C:/Users/mramire8/Documents/Research/Oracle confidence and Interruption/dataset/aclImdb/raw-data'\n",
    "sraa_path = '/Users/maru/MyCode/data/sraa'\n",
    "\n",
    "vct2 = exputil.get_vectorizer({'vectorizer':'bow', 'limit':None, 'min_size':2})\n",
    "\n",
    "sraa =  load_dataset(\"sraa\",sraa_path, keep_subject=True)\n",
    "\n",
    "sraa.train.bow = vct2.fit_transform(sraa.train.data)\n",
    "sraa.test.bow = vct2.transform(sraa.test.data)\n",
    "\n",
    "# imdb.test.bow = vct.transform(imdb.test.data)\n",
    "\n",
    "expert_config = {'type'           : \"noisyreluctantscale\", \n",
    "    'model'          : \"lrl1\",\n",
    "    'parameter'      : 0.01,\n",
    "    'costmodel'      : 1,\n",
    "    'sent_tokenizer' : \"first1snippet\",\n",
    "    'snip_size'      : (1,1),\n",
    "    'threshold'      : .3,\n",
    "    'scale': 0.\n",
    "}\n",
    "sraa_expert = exputil.get_expert(expert_config, size=len(sraa.train.data))\n",
    "sraa_expert.fit(sraa.train.data, y=sraa.train.target, vct=vct2)\n",
    "\n",
    "errors2 = [0.,0.138,0.276,0.414,0.552,0.690] # scale for IMDB\n",
    "cost2=np.array([5.2, 6.5, 7.6,9.1,10.3])\n",
    "test_level(sraa, sraa_expert, vct2, 1, cost2, kvalues, errors2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
