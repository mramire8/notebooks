{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMT Label for Sentences of Movie Reviews\n",
    "\n",
    "## Data Results\n",
    "\n",
    "The results of all batches of labeling were saved in a CSV files including a document and sentences id (from docid split) \n",
    "\n",
    "## Objective\n",
    "\n",
    "We want to determine how resolving conflicts will affect the label distribution of the results \n",
    "\n",
    "## Methods\n",
    "\n",
    "### 1. Default to Neutral\n",
    "\n",
    "This method solves conflicts by default to neutral label, whether there is a neutral answer or conflict the label becomes neutral. \n",
    "\n",
    "### 2. Default to Neutral with Coin Tie-breaker\n",
    "\n",
    "This method return neutral if at least one answer is neutral, if the answers are not neutral it flips a coin to answer. \n",
    "\n",
    "### 3. Default to Label with Coint Tie-breaker\n",
    "\n",
    "This method returns a label if at least one non-neutral answer is available, otherwise flip a coin to answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Imports \n",
    "%matplotlib inline\n",
    "\n",
    "STRUCTURED = '/Users/maru/MyCode/structured'\n",
    "DATA= 'C:/Users/mramire8/Dropbox/My Papers/Structured Reading/Code/Data/sample3_v3_merge/'\n",
    "DATA= '/Users/maru/Dropbox/My Papers/Structured Reading/Code/Data/sample3_v3_merge/'\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data_results(filename):\n",
    "    import csv \n",
    "    from collections import defaultdict\n",
    "    results = defaultdict(lambda : [])\n",
    "    header = []\n",
    "    with open(filename, 'rb') as csvfile:\n",
    "        sents = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in sents:\n",
    "            \n",
    "            for k,v in row.items():\n",
    "                results[k].append(v)\n",
    "            \n",
    "    return results\n",
    "\n",
    "amt = load_data_results(DATA + \"amt.results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement\n",
      "Answer\n",
      "Answer1\n",
      "Answer2\n",
      "DOCID\n",
      "Date\n",
      "HITID\n",
      "ID\n",
      "SENTID\n",
      "TARGET\n",
      "TEXT\n",
      "Worker1\n",
      "Worker2\n"
     ]
    }
   ],
   "source": [
    "print \"\\n\".join(sorted(amt.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_distribution(targets, label=None ):\n",
    "    ''' Calculate the counts of label in targets'''\n",
    "    from collections import Counter\n",
    "    c = Counter(targets)\n",
    "    return c\n",
    "\n",
    "def answer_to_label(ans):\n",
    "    if isinstance(ans, int):\n",
    "        return ans \n",
    "    if 'Negative' in ans:\n",
    "        return 0\n",
    "    elif 'Positive'in ans:\n",
    "        return 1\n",
    "    elif 'Neutral'in ans:\n",
    "        return 2\n",
    "    else: \n",
    "        return 3\n",
    "\n",
    "def to_label(targets):\n",
    "    return [answer_to_label(t) for t in targets]\n",
    "    \n",
    "def print_dist(dist):\n",
    "    return \"\\n\".join([\"%s: %s - %.3f\" % (k,v, 1.*v/sum(dist.values())) for k,v in dist.items()])\n",
    "\n",
    "\n",
    "def to_answers(data, conflict_solver, rnd):\n",
    "    return np.array([solve_conflict(a,b,conflict_solver, rnd) for a,b in zip(data['Answer1'], data['Answer2'])])\n",
    "\n",
    "def solve_conflict(a1, a2, conflict_fn, rnd):\n",
    "    if a1 != a2:\n",
    "        a11 = answer_to_label(a1)\n",
    "        a22 = answer_to_label(a2)\n",
    "        return conflict_fn(a11, a22, rnd)\n",
    "    else:\n",
    "        return answer_to_label(a1)\n",
    "\n",
    "def solver_allneutral(a1, a2, rnd):\n",
    "    if a1 != a2:\n",
    "        return answer_to_label('Neutral')\n",
    "    else: \n",
    "        return a1\n",
    "\n",
    "def solver_neutral(a1, a2, rnd):\n",
    "    if (a1 + a2) < 2: \n",
    "        #flip a coin\n",
    "        return rnd.randint(2)\n",
    "    else: # if  there is a neutral in the answers\n",
    "        return 2 # return neutral\n",
    "\n",
    "def solver_label(a1, a2, rnd):\n",
    "    if (a1 + a2) < 2: \n",
    "        #flip a coin\n",
    "        return rnd.randint(2)\n",
    "    else: # if there is a neutral in the answer\n",
    "        return min(a1, a2) # return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(true, pred):\n",
    "    pred\n",
    "    nonneutral= pred < 2\n",
    "    return metrics.accuracy_score(true[nonneutral], pred[nonneutral])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Distribution \n",
    "\n",
    "We tested the conflict resolution methods and computed the label distribution. Base is the original distribution before resolving conflicts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " === True Distribution ===\n",
      "1: 1460 - 0.512\n",
      "0: 1392 - 0.488\n",
      "\n",
      "=== Base distribution ===\n",
      "0: 682 - 0.239\n",
      "1: 702 - 0.246\n",
      "2: 719 - 0.252\n",
      "3: 749 - 0.263\n",
      "\n",
      "== All neutral ==\n",
      "0: 682 - 0.239\n",
      "1: 702 - 0.246\n",
      "2: 1468 - 0.515\n",
      "Accuracy: 0.865606936416\n",
      "\n",
      "== Neutral-Coin neutral ==\n",
      "0: 736 - 0.258\n",
      "1: 753 - 0.264\n",
      "2: 1363 - 0.478\n",
      "Accuracy: 0.840161182001\n",
      "\n",
      "== Label-Coin neutral ==\n",
      "0: 1076 - 0.377\n",
      "1: 1057 - 0.371\n",
      "2: 719 - 0.252\n",
      "Accuracy: 0.791842475387\n"
     ]
    }
   ],
   "source": [
    "# print amt['Answer1']\n",
    "base =  to_label(amt['Answer'])\n",
    "print \"\\n === True Distribution ===\"\n",
    "c = label_distribution(amt['TARGET'])\n",
    "print print_dist(c)\n",
    "print \"\\n=== Base distribution ===\"\n",
    "true = to_label(amt['Answer'])\n",
    "print print_dist(label_distribution(true))\n",
    "rnd = np.random.RandomState(123)\n",
    "allneu = to_answers(amt, solver_allneutral, rnd)\n",
    "\n",
    "true = np.array(to_label(int(a) for a in amt['TARGET']))\n",
    "\n",
    "print \"\\n== All neutral ==\"\n",
    "print print_dist(label_distribution(allneu))\n",
    "print \"Accuracy:\", accuracy(true, allneu)\n",
    "\n",
    "rnd = np.random.RandomState(555)\n",
    "neu = to_answers(amt, solver_neutral, rnd)\n",
    "print \"\\n== Neutral-Coin neutral ==\"\n",
    "print print_dist(label_distribution(neu))\n",
    "print \"Accuracy:\", accuracy(true, neu)\n",
    "\n",
    "rnd = np.random.RandomState(123)\n",
    "lbls = to_answers(amt, solver_label, rnd)\n",
    "print \"\\n== Label-Coin neutral ==\"\n",
    "print print_dist(label_distribution(lbls))\n",
    "print \"Accuracy:\", accuracy(true, lbls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Confusion matrix of the labels after resolving conflicts for each methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONFUSION MATRIX ===\n",
      "Predicted -->\n",
      "\n",
      "\n",
      "              == All neutral ==              \n",
      "               neg      pos      neu\n",
      "      neg   607.00   111.00   674.00\n",
      "      pos    75.00   591.00   794.00\n",
      "      neu     0.00     0.00     0.00\n",
      "\n",
      "               neg      pos      neu\n",
      "      neg     0.21     0.04     0.24\n",
      "      pos     0.03     0.21     0.28\n",
      "      neu     0.00     0.00     0.00\n",
      "\n",
      "         == Neutral-Coin neutral ==          \n",
      "               neg      pos      neu\n",
      "      neg   642.00   144.00   606.00\n",
      "      pos    94.00   609.00   757.00\n",
      "      neu     0.00     0.00     0.00\n",
      "\n",
      "               neg      pos      neu\n",
      "      neg     0.23     0.05     0.21\n",
      "      pos     0.03     0.21     0.27\n",
      "      neu     0.00     0.00     0.00\n",
      "\n",
      "          == Label-Coin neutral ==           \n",
      "               neg      pos      neu\n",
      "      neg   870.00   238.00   284.00\n",
      "      pos   206.00   819.00   435.00\n",
      "      neu     0.00     0.00     0.00\n",
      "\n",
      "               neg      pos      neu\n",
      "      neg     0.31     0.08     0.10\n",
      "      pos     0.07     0.29     0.15\n",
      "      neu     0.00     0.00     0.00\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix of each method\n",
    "# print amt['Answer1']\n",
    "def print_cm(cm):\n",
    "    labels =['neg', 'pos', 'neu']\n",
    "\n",
    "    row_format =\"{:>9}\" * (len(labels) + 1)\n",
    "    print row_format.format(\"\", *labels)\n",
    "    for lbl, row in zip(labels, cm):\n",
    "        print row_format.format(lbl, *[\"{:.2f}\".format(r) for r in row])\n",
    "    #     return \"\\n\".join([\"{0:.2f}\\t{1:.2f}\\t{2:.2f}\".format(*r) for r in cm])\n",
    "\n",
    "print \"\\n=== CONFUSION MATRIX ===\"\n",
    "print \"Predicted -->\\n\"\n",
    "print \"\\n{0:^45}\".format(\"== All neutral ==\")\n",
    "cm = metrics.confusion_matrix(true, allneu, labels=[0,1,2])\n",
    "print_cm(cm)\n",
    "print \n",
    "print_cm(1. * cm / cm.sum())\n",
    "\n",
    "print \"\\n{0:^45}\".format(\"== Neutral-Coin neutral ==\")\n",
    "cm = metrics.confusion_matrix(true, neu, labels=[0,1,2])\n",
    "print_cm(cm)\n",
    "print \n",
    "print_cm(1. * cm / cm.sum())\n",
    "\n",
    "cm = metrics.confusion_matrix(true, lbls, labels=[0,1,2])\n",
    "print \"\\n{0:^45}\".format(\"== Label-Coin neutral ==\")\n",
    "print_cm(cm)\n",
    "print \n",
    "print_cm(1. * cm / cm.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D0S0' 'D0S1' 'D0S2' ..., 'D9S4' 'D9S5' 'D9S6']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "ordered = defaultdict(lambda: {})\n",
    "\n",
    "order = np.argsort(amt['ID'])\n",
    "print np.array(amt['ID'])[order]\n",
    "\n",
    "for i in order:\n",
    "    ordered[amt['DOCID'][i]][amt['SENTID'][i][1:]] = amt['TEXT'][i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '10', '11', '12', '13', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(int(k) for k in ordered['D1'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(ordered['D3'], key=lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Usually musicals in the 1940's were of a set formula - and if you studied films you know what I'm talking about - a certain running lenghth, very 'showy' performances that were great on the surface but never got into the real personalities of the characters etc.\",\n",
       " \"THIS ONE IS DIFFERENT - and light years better and well worth it's nomination for best picture of the year - 1945 (although had no chance of beating the eventual winner - Lost Weekend).\",\n",
       " \"Gene Kelly was probably in the best form of his career - yes I know about 'American in Paris' and 'Singing in the Rain'.\",\n",
       " 'This one is different.',\n",
       " \"He really gets into his character of a 'sea wolf' thinking (at first) that 'picking up any girl while on leave' is nothing more than a lark.\",\n",
       " \"And if you had to make up a 'story' to get her - so be it - until.\",\n",
       " \"Sort of like the Music Man when he gets 'his foot caught in the door'.\",\n",
       " \"The eventual hilarity of the film stems mostly from his and his new pal (Sinatra)'s attempt to make the 'story' good in order to 'get the girl' that he REALLY and unexpectedly falls in love with.\",\n",
       " 'You are going to have to see the movie to see what I mean.',\n",
       " \"Besides that there are so many other elements of great film in this one, it's a classic buddy story, nostalgia to a time when WWII was almost over (the war ended about a month after the films release), a realization that a guy that always laughed at life can find out that he really is a great human being, great songs and probably a few other elements of classic film making that I can't think of right now.\",\n",
       " 'Why not a 10?',\n",
       " 'Near the end - at nearly 2 1/2 hours starts to feel a bit long.',\n",
       " 'There is a small ballet number that Gene Kelly does that must have been a sensation in 1945 but seems dated and feels like it just adds minutes now.',\n",
       " 'But overall, this ones a definite winner on every level.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ordered['D1'][txt] for txt in sorted(ordered['D1'], key=lambda x: int(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(amt['DOCID']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
