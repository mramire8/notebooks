{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classifier with Feature Template\n",
    "\n",
    "## Settings\n",
    "\n",
    "### Feature Function\n",
    "The feature function $\\mathbf{f}$ uses the following features to represent a sentence: \n",
    "\n",
    "* Features for sentence context, each sentnece and its neighbors will have the following:\n",
    "    * Number of tokens\n",
    "    * Number of positive, negative and neutral tokens\n",
    "    * Proportion of positive over negative\n",
    "    * Proportion of negative over postive\n",
    "    * Proportion of neutral \n",
    "* Feature for document context:\n",
    "    * Same as the sentences but for the full document\n",
    "* Labels\n",
    "    * Sentence level label from DaS classifier ($y_i^s$)\n",
    "    \n",
    "\n",
    "### Training the Classifier\n",
    "\n",
    "We select N random documents and train a DaS classifier (trained on documents) to predict the label of the sentence $y^s_i$. We create a logistic regression classifier that will be trained on data using the feature fucntion representation $P_E(y^s|\\mathbf{f}(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Imports \n",
    "%matplotlib inline\n",
    "\n",
    "STRUCTURED = '/Users/maru/MyCode/structured'\n",
    "IMDB_DATA='/Users/maru/MyCode/data/imdb'\n",
    "SRAA_DATA='/Users/maru/MyCode/data/sraa'\n",
    "TWIITER_DATA = '/Users/maru/MyCode/data/twitter'\n",
    "\n",
    "# IMDB_DATA = 'C:/Users/mramire8/Documents/Research/Oracle confidence and Interruption/dataset/aclImdb/raw-data'\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(STRUCTURED))\n",
    "sys.path.append(os.path.abspath('C:/cygwin/home/mramire8/python_code/structured/'))\n",
    "\n",
    "import learner\n",
    "\n",
    "from utilities.datautils import load_dataset\n",
    "import experiment.base as exp\n",
    "\n",
    "\n",
    "import utilities.experimentutils as exputil\n",
    "import utilities.datautils as datautil\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get the data ready\n",
    "import re \n",
    "vct = CountVectorizer(min_df=2, token_pattern=re.compile(r'(?u)\\b\\w+\\b'))\n",
    "\n",
    "# vct_doc = CountVectorizer(encoding='ISO-8859-1', min_df=2, max_df=1.0, binary=True, token_pattern='\\\\b\\\\w+\\\\b')\n",
    "vct_doc = exputil.get_vectorizer({'vectorizer':'bow', 'limit':None, 'min_size':2})\n",
    "\n",
    "\n",
    "\n",
    "sent_tk = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "imdb =  load_dataset(\"imdb\",IMDB_DATA, keep_subject=True)\n",
    "\n",
    "imdb.train.bow = vct_doc.fit_transform(imdb.train.data)\n",
    "imdb.test.bow = vct_doc.transform(imdb.test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document(object):\n",
    "    def __init__(self, raw_text, lbl, sent_tk, vct_gral, sent_lbl=None):\n",
    "        sentences = sent_tk.tokenize_sents([raw_text])[0]\n",
    "        self.doc_label = lbl\n",
    "        self.sent_bow = vct_gral.transform(sentences) # counts per sentence\n",
    "        if sent_lbl is not None:\n",
    "            self.sent_labels = [lbl] * len(sentences)\n",
    "        else:\n",
    "            self.sent_labels = sent_lbl#np.array([s.split('\\t')[0] for s in self.sentences])\n",
    "            \n",
    "    def __init__(self,  sents_bow, sents_lbl, doc_lbl):\n",
    "#         self.sentences = sents\n",
    "        self.doc_label = doc_lbl\n",
    "        self.sent_labels = sents_lbl\n",
    "        self.sent_bow = sents_bow\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_sentences(documents):\n",
    "    for d in documents:\n",
    "        for s in d:\n",
    "            yield s\n",
    " \n",
    "    \n",
    "def load_documents(data, vct, sent_tk ):\n",
    "    # sents, sents_bow, sents_lbl, doc_lbl\n",
    "    \n",
    "    sents_doc = sent_tk.tokenize_sents(data.data)\n",
    "    sents_bow = [vct.transform(d) for d in sents_doc]\n",
    "    sents_lbl = [[l]*len(s) for l,s in  zip(data.target, sents_doc)]\n",
    "    \n",
    "    x = np.array([Document(a,b,c) for a,b,c in zip( sents_bow, sents_lbl, data.target)])\n",
    "    y = data.target\n",
    "    return x,y\n",
    "\n",
    "def load_documents_v2(data, vct, sent_tk, doc_clf ):\n",
    "    # sents, sents_bow, sents_lbl, doc_lbl\n",
    "    \n",
    "    sents_doc = sent_tk.tokenize_sents(data.data)\n",
    "\n",
    "    X = vec.fit_transform(iterate_sentences(sents_doc))\n",
    "    start = 0\n",
    "    sents_bow = []\n",
    "    for d in sents_doc:\n",
    "        end = start + len(d)\n",
    "        sents_bow.append(X[start:end])\n",
    "        start = end\n",
    "\n",
    "#     sents_bow = [vct.transform(d) for d in sents_doc]\n",
    "    sents_lbl = [[l]*len(s) for l,s in  zip(data.target, sents_doc)]\n",
    "    \n",
    "    x = np.array([Document(a,b,c) for a,b,c in zip( sents_bow, sents_lbl, data.target)])\n",
    "    y = data.target\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def features_per_document(doc, doc_clf, feature_fn, top=10, threshold=.47):\n",
    "    x = np.vstack((feature_fn(doc, i, doc_clf, top=top, threshold=threshold) for i in range(len(doc.sent_labels))))\n",
    "    return x\n",
    "\n",
    "def get_training_sentence(documents, doc_clf, feature_fn, top=10, threshold=.47):\n",
    "    x = np.vstack((features_per_document(d, doc_clf, feature_fn, top=top, threshold=threshold) for d in documents))\n",
    "    return x[:,:-1], x[:,-1]\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x,y = load_documents(imdb.train, vct_doc, sent_tk)\n",
    "\n",
    "\n",
    "doc_clf = LogisticRegression(penalty='l1', C=1)\n",
    "doc_clf.fit(imdb.train.bow, imdb.train.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-doc accuracy: 0.8884\n",
      "Test size: 24989\n"
     ]
    }
   ],
   "source": [
    "# Testing document classifier\n",
    "\n",
    "print \"Document-doc accuracy: %.4f\" % metrics.accuracy_score(imdb.test.target, doc_clf.predict(imdb.test.bow))\n",
    "print \"Test size: %s\" % imdb.test.bow.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-sentence accuracy: 0.6503\n",
      "Test size: 304952\n"
     ]
    }
   ],
   "source": [
    "# Testing Document-sentence \n",
    "# Sentences take label of the document\n",
    "\n",
    "def doc_to_sents(docs_text, doc_labels, vct):\n",
    "    '''Create bow features for sentences of one document'''\n",
    "    \n",
    "    sent_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    doc_sents = sent_splitter.tokenize_sents(docs_text)\n",
    "    sizes = [len(d) for d in doc_sents]\n",
    "    labels = [[l]*s for l, s in zip(doc_labels, sizes)]\n",
    "    \n",
    "    sents_bow = vct.transform(iterate_sentences(doc_sents))\n",
    "    labels = np.array([l for l in iterate_sentences(labels)])\n",
    "    \n",
    "    return sents_bow, labels\n",
    "\n",
    "test_sx, test_sy = doc_to_sents(imdb.test.data, imdb.test.target, vct_doc)\n",
    "print \"Document-sentence accuracy: %.4f\" % metrics.accuracy_score(test_sy, doc_clf.predict(test_sx))\n",
    "print \"Test size: %s\" % len(test_sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent-doc accuracy: 0.9004\n",
      "Sent-sentence accuracy: 0.6982\n",
      "Test size: 304952\n"
     ]
    }
   ],
   "source": [
    "# Testing sentence to sentence\n",
    "train_sx, train_sy = doc_to_sents(imdb.train.data, imdb.train.target, vct_doc)\n",
    "s2s_clf = LogisticRegression(penalty='l1', C=1)\n",
    "s2s_clf.fit(train_sx, train_sy)\n",
    "print \"Sent-doc accuracy: %.4f\" % metrics.accuracy_score(imdb.test.target, s2s_clf.predict(imdb.test.bow))\n",
    "print \"Sent-sentence accuracy: %.4f\" % metrics.accuracy_score(test_sy, s2s_clf.predict(test_sx))\n",
    "print \"Test size: %s\" % len(test_sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perc. neutrals: 0.241426166735\n",
      "Accuracy: 0.717986287853\n"
     ]
    }
   ],
   "source": [
    "## Neutrality and accuracy of the oracle on sentences, on training data\n",
    "pred_prob = doc_clf.predict_proba(train_sx)\n",
    "pred_sent = doc_clf.predict(train_sx)\n",
    "unc_sent = 1- pred_prob.max(axis=1)\n",
    "thres = 0.42\n",
    "pred_sent[unc_sent > thres] = 2\n",
    "print \"Perc. neutrals: %s\" % (1. * len(pred_sent[unc_sent > thres]) / len(pred_sent))\n",
    "non_neu = pred_sent < 2\n",
    "print \"Accuracy: %s\" % (metrics.accuracy_score(np.array(train_sy)[non_neu], pred_sent[non_neu]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,  TransformerMixin,ClassifierMixin\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack, hstack\n",
    "\n",
    "\n",
    "class ContextVectorizer(BaseEstimator, TransformerMixin):\n",
    "# class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for Vectorizer\"\"\"\n",
    "    def __init__(self, doc_clf, vct, feature_fn,top=10, threshold=.47):\n",
    "        self.doc_clf = doc_clf\n",
    "        self.vct = vct\n",
    "        self.feature_fn = feature_fn \n",
    "        self.top = top\n",
    "        self.threshold = threshold \n",
    "        self.feature_fn = self.feature_context\n",
    "        self.lexicon =  self.get_lexicon(self.doc_clf, top=self.top)\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        x = self.get_training_sentence(documents,  self.feature_fn, \n",
    "                                       threshold=self.threshold)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_lexicon(self, clf, top=10):\n",
    "        '''\n",
    "        Return lexicon of top K terms according to classifier clf. \n",
    "        The function returns feat_index-class pairs\n",
    "        '''\n",
    "    #     feats = np.array(vct.get_feature_names())\n",
    "        coefs = clf.coef_\n",
    "        if coefs.shape[0] == 1:\n",
    "            coefs = [-1 * coefs[0], coefs[0]]\n",
    "\n",
    "        res = []\n",
    "        for ci, cname in enumerate(clf.classes_): # for every class\n",
    "            coef = coefs[ci]\n",
    "            res.extend([(i, cname) for i in np.argsort(coef)[::-1][:top]])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def get_context(self, doc, i):\n",
    "        ''' Get surrounding sentences for context '''\n",
    "        zero_mat = csr_matrix(doc.sent_bow[0].shape, dtype=np.int64)\n",
    "        if i >= len(doc.sent_labels):\n",
    "            raise Exception(\"This doc is not that long.\")\n",
    "        if len(doc.sent_labels) == 1: # there is only one sentence\n",
    "            return np.array([zero_mat, doc.sent_bow[0], zero_mat])\n",
    "        elif i==0: # for the first1\n",
    "            return np.array([zero_mat, doc.sent_bow[0], doc.sent_bow[1]])\n",
    "        elif i == len(doc.sent_labels)-1: # for the last one \n",
    "            return np.array([doc.sent_bow[i-1], doc.sent_bow[i], zero_mat])\n",
    "        else: \n",
    "            return np.array([doc.sent_bow[i-1], doc.sent_bow[i], doc.sent_bow[i+1]])\n",
    "\n",
    "    def get_sentence_label(self,  x, threshold=.4):\n",
    "        '''Get a label or a neutral answer by uncertainty threshold.'''\n",
    "\n",
    "        unc = 1 - self.doc_clf.predict_proba(x).max()\n",
    "        if unc < threshold :\n",
    "            return self.doc_clf.predict(x)\n",
    "        else:\n",
    "            return np.array([2]) ## Neutral label class\n",
    "\n",
    "    def feature_context(self, doc, i,  threshold=.47):\n",
    "        '''Feature function, context and lexicon counts, for one sentence '''\n",
    "        context = self.get_context(doc, i)\n",
    "        sent_lbl = self.get_sentence_label(doc.sent_bow[i], threshold=threshold)\n",
    "\n",
    "        n_lex = len(self.lexicon)\n",
    "        n_feat = (3 * n_lex) + 1  # 2 context sentences and current sentence + label\n",
    "        lex_index = [x[0] for x in self.lexicon]\n",
    "\n",
    "        # Add context sentences + Add sentence label, predicted\n",
    "        lbl_mat = csr_matrix((sent_lbl, ([0] , [0])), shape=(1, 1), dtype=np.int64)\n",
    "        new_feat =  hstack([c[:,lex_index] for c in context]+[lbl_mat])\n",
    "\n",
    "        if new_feat.shape[1] != n_feat:\n",
    "            raise ValueError(\"Oops, convertion did not work. The features are no correct.\")\n",
    "\n",
    "        return new_feat\n",
    "    \n",
    "\n",
    "    def features_per_document(self, doc,  feature_fn,  threshold=.47):\n",
    "        x = vstack([self.feature_fn(doc, i, threshold=threshold) for i in range(len(doc.sent_labels))])\n",
    "        return x\n",
    "\n",
    "    def get_training_sentence(self, documents,  feature_fn,  threshold=.47):\n",
    "        x = vstack([self.features_per_document(d, feature_fn,  threshold=threshold) for d in documents])\n",
    "        return x\n",
    "\n",
    "    def set_top(top):\n",
    "        self.top = top\n",
    "        \n",
    "    def set_unc_threshold(thr):\n",
    "        self.threshold = thr \n",
    "        \n",
    "        \n",
    "class SentenceClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Sentence Classifier. Takes data from ContextVectorizer\"\"\"\n",
    "    def __init__(self):\n",
    "        self.clf = LogisticRegression(penalty='l1', C=1)\n",
    "        \n",
    "        \n",
    "    def __init__(self, doc_clf, vct, feature_fn,top=10, threshold=.47):\n",
    "        self.clf = LogisticRegression(penalty='l1', C=1)\n",
    "        self.converter = ContextVectorizer(doc_clf, vct, feature_fn, top=top, threshold=threshold)\n",
    "        \n",
    "    def convert(self, x):\n",
    "        xx = self.converter.transform(x)\n",
    "        return xx\n",
    "    \n",
    "    def fit(self, x,y):\n",
    "        xx = self.convert(x)\n",
    "        self.clf.fit(xx, y)\n",
    "        self.classes_ = self.clf.classes_\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        xx = self.convert(X)\n",
    "\n",
    "        return self.clf.predict(xx)\n",
    "\n",
    "    def fit2(self, X, y):\n",
    "        self.clf.fit(X,y)\n",
    "        self.classes_ = self.clf.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict2(self, X):\n",
    "        return self.clf.predict(X)  \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        xx = self.convert(X)\n",
    "        return self.clf.predict_proba(xx)\n",
    "\n",
    "    def predict_proba2(self, X):\n",
    "        return self.clf.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24989\n"
     ]
    }
   ],
   "source": [
    "## Convert testing documents \n",
    "te_x, te_y = load_documents(imdb.test, vct_doc, sent_tk)\n",
    "print \"%s\" % (len(te_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "\n",
    "* create a training funtion for data with label from te document \n",
    "* create a function for data from amt with original labels (should work for other data as well)\n",
    "* create a fnction for testing same as training options \n",
    "* create a cv test \n",
    "* create a plot with cv\n",
    "* test base liens\n",
    "* test classifiers\n",
    "* test classifier with simple fieatures\n",
    "* with features for sentiment analysis \n",
    "* with fancy features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accu': 0.96721311475409832, 'test_size': 122, 'neutrals': 0.0}\n"
     ]
    }
   ],
   "source": [
    "## Training with Documents\n",
    "def extract_y(x):\n",
    "    yy = []\n",
    "    for d in x:\n",
    "        yy.extend(d.sent_labels)\n",
    "    yy = np.array(yy)\n",
    "    return yy\n",
    "    \n",
    "def train_clf(x, clf, convert=False):\n",
    "    yy = extract_y(x)\n",
    "    if convert:\n",
    "        clf.fit(x, yy)\n",
    "    else:\n",
    "        clf.fit2(x,yy)\n",
    "        \n",
    "    return clf\n",
    "\n",
    "def test_clf(x, y, clf, convert=False):\n",
    "    '''\n",
    "    Effective accuracy and neutrality percentage test\n",
    "    :param x: list of Document instances\n",
    "    '''\n",
    "    if y is None:\n",
    "        yy = extract_y(x)\n",
    "    else:\n",
    "        yy = y\n",
    "    \n",
    "    if convert:\n",
    "        pred = clf.predict(x)\n",
    "    else:\n",
    "        pred = clf.predict2(x)\n",
    "\n",
    "    non_neu = pred < 2\n",
    "    return {'accu':metrics.accuracy_score(yy[non_neu], pred[non_neu]), 'neutrals':1- 1.* sum(non_neu) / len(pred), 'test_size':len(pred)}\n",
    "\n",
    "s = SentenceClassifier(doc_clf, vct_doc, None, top=2500, threshold=.42)\n",
    "ss = train_clf(x[:10], s, convert=True)\n",
    "print test_clf(x[:10], ss, convert=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get learning curve data\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "\n",
    "def experiment(data, train_fn, test_fn, clf, train_sizes=np.linspace(.1,1.,5), n_folds=5, seed=12222):\n",
    "#     clf = SentenceClassifier(doc_clf, vct_doc, None, top=2500, threshold=.42)\n",
    "#     cv = cv.ShuffleSplit(len(y), n_iter=5, test_size=.0, random_state=12345)\n",
    "    cross_val = cv.KFold(len(data), n_folds=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    test_scores = []\n",
    "        \n",
    "    for train_index, test_index in cross_val:\n",
    "        trial_score = []\n",
    "        print \"Train size: %s, Test size: %s\" % (len(train_index), len(test_index))\n",
    "        \n",
    "        # convert test once\n",
    "        test_x = clf.convert(x[test_index])\n",
    "        test_y = extract_y(x[test_index])\n",
    "\n",
    "        # convert train once \n",
    "        sent_size = [d.sent_bow.shape[0] for d in x[train_index[:max(train_sizes)]]]\n",
    "        train_x = clf.convert(x[train_index[:max(train_sizes)]])\n",
    "\n",
    "        for size in train_sizes:\n",
    "            print \"Size: %s\" % size\n",
    "            adj_size= sum(sent_size[:size])\n",
    "\n",
    "#             trained = train_fn(x[:size], clf, convert=True)\n",
    "\n",
    "            trained = train_fn(train_x[:adj_size], train_y[:adj_size],clf, convert=False)\n",
    "            trial_score.append(test_fn(test_x, test_y, trained,convert=False))\n",
    "            print \"Accuracy: %s\" % trial_score[-1]\n",
    "        test_scores.append(trial_score)\n",
    "    \n",
    "    test_scores_mean = np.mean([t['accu'] for t in test_scores], axis=0)\n",
    "    test_scores_std = np.std([t['accu'] for t in test_scores], axis=0)\n",
    "    \n",
    "\n",
    "    return test_scores, test_scores_mean, test_scores_std \n",
    "\n",
    "def experiment_v2(x, train_fn, test_fn, clf, train_sizes=np.linspace(.1,1.,5), n_folds=5, seed=12222, sent_size=None):\n",
    "#     clf = SentenceClassifier(doc_clf, vct_doc, None, top=2500, threshold=.42)\n",
    "#     cv = cv.ShuffleSplit(len(y), n_iter=5, test_size=.0, random_state=12345)\n",
    "    cross_val = cv.KFold(len(data), n_folds=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    test_scores = []\n",
    "\n",
    "    for train_index, test_index in cross_val:\n",
    "        trial_score = []\n",
    "        \n",
    "        for size in train_sizes:\n",
    "            print \"Size: %s\" % size\n",
    "            adj_size= sum(sent_size[:size])\n",
    "\n",
    "#             trained = train_fn(x[:size], clf, convert=True)\n",
    "\n",
    "            trained = train_fn(train_x[:adj_size], train_y[:adj_size],clf, convert=False)\n",
    "            trial_score.append(test_fn(test_x, test_y, trained,convert=False))\n",
    "            print \"Accuracy: %s\" % trial_score[-1]\n",
    "        test_scores.append(trial_score)\n",
    "    \n",
    "    test_scores_mean = np.mean([t['accu'] for t in test_scores], axis=0)\n",
    "    test_scores_std = np.std([t['accu'] for t in test_scores], axis=0)\n",
    "    \n",
    "\n",
    "    return test_scores, test_scores_mean, test_scores_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-87f478c6aebb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%timeit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msent_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-d138b3c82b74>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         x = self.get_training_sentence(documents,  self.feature_fn, \n\u001b[0;32m---> 23\u001b[0;31m                                        threshold=self.threshold)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-d138b3c82b74>\u001b[0m in \u001b[0;36mget_training_sentence\u001b[0;34m(self, documents, feature_fn, threshold)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_training_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfeature_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.47\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_per_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-d138b3c82b74>\u001b[0m in \u001b[0;36mfeatures_per_document\u001b[0;34m(self, doc, feature_fn, threshold)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeatures_per_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfeature_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.47\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-d138b3c82b74>\u001b[0m in \u001b[0;36mfeature_context\u001b[0;34m(self, doc, i, threshold)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Add context sentences + Add sentence label, predicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlbl_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_lbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mnew_feat\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlex_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlbl_mat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_feat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \"\"\"\n\u001b[0;32m--> 456\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m                 \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m                 \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m                 \u001b[0mblock_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/coo.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mcoo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36mtocoo\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/coo.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/coo.pyc\u001b[0m in \u001b[0;36m_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_native\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnnz\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/sputils.pyc\u001b[0m in \u001b[0;36mto_native\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_native\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewbyteorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'native'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \"\"\"\n\u001b[0;32m--> 474\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "train_x = s.converter.transform(x)\n",
    "train_y = extract_y(x[test_index])\n",
    "sent_len = [d.sent_bow.shape[0] for d in x]\n",
    "print train_x.shape\n",
    "print train_y.shape\n",
    "print len(sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exp_plot(results, sizes=None):\n",
    "    \n",
    "    for m, res in results.items():\n",
    "        avg = res[1]\n",
    "        std = res[2]\n",
    "        plt.plot(sizes,avg, label=m)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel('Accuracy of Student')\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizes = [100, 250, 500, 1000, 2000, 3000]\n",
    "sizes = [10, 20]\n",
    "res1 = experiment(x, train_clf, test_clf, SentenceClassifier(doc_clf, vct_doc, None, top=2500, threshold=.42), \n",
    "                  train_sizes=sizes, n_folds=5, seed=12222)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s.fit(x[:100], None)\n",
    "s.predict(x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x[0].sent_bow[0].shape\n",
    "csr_matrix(([0],[0] , []), shape=(1, 1), dtype=np.int64).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Feature Spaces \n",
    "\n",
    "## 1. Sentiment-based Features\n",
    "\n",
    "Based on (McDonald, 2011): \n",
    "\n",
    "Features are s :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train sentence classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "context_vect = ContextVectorizer(doc_clf, vct_doc, feature_context ,top=2500, threshold=.45)\n",
    "sent_cla = SentenceClassifier()\n",
    "\n",
    "## Get the training data\n",
    "# Get all and train classifier, fully trained\n",
    "ss2s_clf = Pipeline(steps=[('context', context_vect),('estimator', sent_cla)])\n",
    "ss2s_clf.fit(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Get the test data\n",
    "# Transform all test\n",
    "text_x,test_y = load_documents(imdb.test, vct_doc, sent_tk)\n",
    "testx_ss2s = context_vect.transform(test_x)\n",
    "\n",
    "# print \"Sent-doc accuracy: %.4f\" % metrics.accuracy_score(testx_ss2s[:,-1], s2s_clf.predict(testx_ss2s[:,:-1]))\n",
    "print \"Sent-sentence accuracy: %.4f\" % metrics.accuracy_score(testx_ss2s[:,-1], s2s_clf.predict(testx_ss2s[:,:-1]))\n",
    "print \"Test size: %s\" % (testx_ss2s.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n",
    "\n",
    "sent_x\n",
    "\n",
    "# # prueba =  context_vect.transform(x[:10])\n",
    "# # print prueba[:,-1]\n",
    "\n",
    "\n",
    "# # prueba2 = get_training_sentence(x[:10], doc_clf, feature_context, top=10, threshold=.47)\n",
    "# # print prueba2[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sent_cla = SentenceClassifier()\n",
    "clf_sent = Pipeline(steps=[('context', context_vect),('estimator', sent_cla)])\n",
    "\n",
    "# clf_sent.fit(sub_x, sub_y)\n",
    "\n",
    "# t_x = context_vect.transform(sub_x)\n",
    "print metrics.accuracy_score(t_x[:,-1], sent_cla.predict(t_x))\n",
    "print metrics.accuracy_score(t_x[:,-1], clf_sent.predict(sub_x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## train on bootstraps, test on amt data sentences\n",
    "#_# train on bootstraps, test on sentences as documents \n",
    "t_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on AMT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "from utilities.amt_datautils import load_amt_imdb\n",
    "amt = load_amt_imdb(IMDB_DATA, shuffle=True, rnd=1928374, amt_labels='labels')  # should bring with training labels as the amt annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_documents_amt(data, vct, sent_tk ):\n",
    "    # sents, sents_bow, sents_lbl, doc_lbl\n",
    "    \n",
    "    sents_doc = [d.split('THIS_IS_A_SEPARATOR') for d in data.data]\n",
    "    sents_bow = [vct.transform(d) for d in sents_doc]\n",
    "    sents_lbl = data.target\n",
    "    \n",
    "    x = np.array([Document(a,b,c,d) for a,b,c,d in zip(sents_doc, sents_bow, sents_lbl, data.doctarget)])\n",
    "    y = data.doctarget\n",
    "    return x,y\n",
    "\n",
    "\n",
    "print amt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print amt.train.keys()\n",
    "print len(amt.train.doctarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert document to new feature space\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import DictVectorizer\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     row = [0] * n_features\n",
    "#     col = [l[0] for l in lexicon]\n",
    "#     counts = d.sent_bow[:,col]\n",
    "    \n",
    "#     data= counts.sum(axis=0)\n",
    "#     return csr_matrix( (data,(row,col)), shape=(1,n_features) )\n",
    "\n",
    "#     csr_matrix()\n",
    "\n",
    "def feature_simple_counts(doc, i, doc_clf):\n",
    "    '''Feature function bag of words, no context'''\n",
    "    return doc.sent_bow[i]\n",
    "    \n",
    "def featurize(documents, labels, clf_d, feature_fn):\n",
    "    '''Create a feature vector from documents'''\n",
    "\n",
    "    lexicon = get_lexicon(clf_d, top=10) \n",
    "    x = vstack((feature_fn(d, lexicon, clf_d) for d in documents))\n",
    "    return x\n",
    "    \n",
    "def feature_counts(doc, lexicon, clf_d):\n",
    "    n_features = len(lexicon) * 4 + 1\n",
    "    row = [0] * n_features\n",
    "    col = [l[0] for l in lexicon]\n",
    "    counts = d.sent_bow[:,col]\n",
    "    \n",
    "    data= counts.sum(axis=0)\n",
    "    return csr_matrix( (data,(row,col)), shape=(1,n_features) )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "1. Load train and test\n",
    "1. Features for train:\n",
    "    1. for documents: vectorizer like before\n",
    "    1. for sentences: lexicon counts\n",
    "1. features for test: same as for sentences (this is amt data)\n",
    "1. for every size of the bootstrap\n",
    "    1. train a document classifier\n",
    "    1. obtain sentences and featurize\n",
    "    1. test document\n",
    "    1. test sentence\n",
    "    1. save results\n",
    "    \n",
    "### Features per sentence\n",
    "\n",
    "1. For every document\n",
    "    1. for every sentence\n",
    "        1. get context, document, and label, and lexicon\n",
    "        1. build a vector\n",
    "        1. return vecotr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def experiment(data, vct, runs, rnd=123):\n",
    "    x_doc, y_doc = load_documents(data, vct)\n",
    "    for train, test in cv:\n",
    "        clf_d = LogisticRegression(penalty=\"l1\", C=1)\n",
    "        clf_s = LogisticRegression(penalty=\"l1\", C=1)\n",
    "        \n",
    "        clf_d.fit(data.train.bow[train], data.train.target[train])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_predict():\n",
    "    pass\n",
    "def lr_fit():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
