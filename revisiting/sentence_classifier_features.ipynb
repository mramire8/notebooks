{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classifier with Feature Template\n",
    "\n",
    "## Settings\n",
    "\n",
    "### Feature Function\n",
    "The feature function $\\mathbf{f}$ uses the following features to represent a sentence: \n",
    "\n",
    "* Features for sentence context, each sentnece and its neighbors will have the following:\n",
    "    * Number of tokens\n",
    "    * Number of positive, negative and neutral tokens\n",
    "    * Proportion of positive over negative\n",
    "    * Proportion of negative over postive\n",
    "    * Proportion of neutral \n",
    "* Feature for document context:\n",
    "    * Same as the sentences but for the full document\n",
    "* Labels\n",
    "    * Sentence level label from DaS classifier ($y_i^s$)\n",
    "    \n",
    "\n",
    "### Training the Classifier\n",
    "\n",
    "We select N random documents and train a DaS classifier (trained on documents) to predict the label of the sentence $y^s_i$. We create a logistic regression classifier that will be trained on data using the feature fucntion representation $P_E(y^s|\\mathbf{f}(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named structured.utilities.datautils",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-118598e409b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlearner\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstructured\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatautils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named structured.utilities.datautils"
     ]
    }
   ],
   "source": [
    "## Imports \n",
    "%matplotlib inline\n",
    "\n",
    "STRUCTURED = '/Users/maru/MyCode/structured'\n",
    "IMDB_DATA='/Users/maru/MyCode/data/imdb'\n",
    "SRAA_DATA='/Users/maru/MyCode/data/sraa'\n",
    "TWIITER_DATA = '/Users/maru/MyCode/data/twitter'\n",
    "\n",
    "IMDB_DATA = 'C:/Users/mramire8/Documents/Research/Oracle confidence and Interruption/dataset/aclImdb/raw-data'\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(STRUCTURED))\n",
    "sys.path.append(os.path.abspath('C:/cygwin/home/mramire8/python_code/structured/'))\n",
    "\n",
    "import learner\n",
    "\n",
    "from utilities.datautils import load_dataset\n",
    "import experiment.base as exp\n",
    "\n",
    "\n",
    "import utilities.experimentutils as exputil\n",
    "import utilities.datautils as datautil\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\cygwin\\home\\mramire8\\python_code\\notebooks\\revisiting\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name _fblas",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b2c1dc6a1880>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatautils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\cygwin\\home\\mramire8\\python_code\\structured\\utilities\\datautils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbunch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\datasets\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \"\"\"\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_diabetes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_digits\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\datasets\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m                          \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                          check_symmetric)\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoblib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\utils\\class_weight.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0min1d\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbincount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msp_version\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;31m# Backport fix for scikit-learn/scikit-learn#2986 / scipy/scipy#4142\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_scipy_sparse_lsqr_backport\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\utils\\_scipy_sparse_lsqr_backport.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maslinearoperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\scipy\\sparse\\linalg\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0misolve\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdsolve\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\scipy\\sparse\\linalg\\isolve\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0miterative\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mminres\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mminres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlgmres\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlgmres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlsqr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlsmr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsmr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\scipy\\sparse\\linalg\\isolve\\lgmres.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_blas_funcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_system\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\scipy\\linalg\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlinalg_version\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg_version\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdecomp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\scipy\\linalg\\misc.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mblas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'LinAlgError'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'norm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\scipy\\linalg\\blas.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_np\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_fblas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_cblas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name _fblas"
     ]
    }
   ],
   "source": [
    "print os.getcwd() \n",
    "\n",
    "from utilities.datautils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get the data ready\n",
    "import re \n",
    "vct = CountVectorizer(min_df=2, token_pattern=re.compile(r'(?u)\\b\\w+\\b'))\n",
    "\n",
    "# vct_doc = CountVectorizer(encoding='ISO-8859-1', min_df=2, max_df=1.0, binary=True, token_pattern='\\\\b\\\\w+\\\\b')\n",
    "vct_doc = exputil.get_vectorizer({'vectorizer':'bow', 'limit':None, 'min_size':2})\n",
    "\n",
    "\n",
    "\n",
    "sent_tk = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "imdb =  load_dataset(\"imdb\",IMDB_DATA, keep_subject=True)\n",
    "\n",
    "imdb.train.bow = vct_doc.fit_transform(imdb.train.data)\n",
    "imdb.test.bow = vct_doc.transform(imdb.test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document(object):\n",
    "    def __init__(self, raw_text, lbl, sent_tk, vct_gral, sent_lbl=None):\n",
    "        sentences = sent_tk.tokenize_sents([raw_text])[0]\n",
    "        self.doc_label = lbl\n",
    "        self.sent_bow = vct_gral.transform(sentences) # counts per sentence\n",
    "        if sent_lbl is not None:\n",
    "            self.sent_labels = [lbl] * len(sentences)\n",
    "        else:\n",
    "            self.sent_labels = sent_lbl#np.array([s.split('\\t')[0] for s in self.sentences])\n",
    "            \n",
    "    def __init__(self,  sents_bow, sents_lbl, doc_lbl):\n",
    "#         self.sentences = sents\n",
    "        self.doc_label = doc_lbl\n",
    "        self.sent_labels = sents_lbl\n",
    "        self.sent_bow = sents_bow\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_sentences(documents):\n",
    "    for d in documents:\n",
    "        for s in d:\n",
    "            yield s\n",
    "\n",
    "def get_lexicon(clf, top=10):\n",
    "    '''\n",
    "    Return lexicon of top K terms according to classifier clf. \n",
    "    The function returns feat_index-class pairs\n",
    "    '''\n",
    "#     feats = np.array(vct.get_feature_names())\n",
    "    coefs = clf.coef_\n",
    "    if coefs.shape[0] == 1:\n",
    "        coefs = [-1 * coefs[0], coefs[0]]\n",
    "        \n",
    "    res = []\n",
    "    for ci, cname in enumerate(clf.classes_): # for every class\n",
    "        coef = coefs[ci]\n",
    "        res.extend([(i, cname) for i in np.argsort(coef)[::-1][:top]])\n",
    "    return res\n",
    " \n",
    "    \n",
    "def load_documents(data, vct, sent_tk ):\n",
    "    # sents, sents_bow, sents_lbl, doc_lbl\n",
    "    \n",
    "    sents_doc = sent_tk.tokenize_sents(data.data)\n",
    "    sents_bow = [vct.transform(d) for d in sents_doc]\n",
    "    sents_lbl = [[l]*len(s) for l,s in  zip(data.target, sents_doc)]\n",
    "    \n",
    "    x = np.array([Document(a,b,c,d) for a,b,c,d in zip( sents_bow, sents_lbl, data.target)])\n",
    "    y = data.target\n",
    "    return x,y\n",
    "\n",
    "def load_documents_v2(data, vct, sent_tk, doc_clf ):\n",
    "    # sents, sents_bow, sents_lbl, doc_lbl\n",
    "    \n",
    "    sents_doc = sent_tk.tokenize_sents(data.data)\n",
    "\n",
    "    X = vec.fit_transform(iterate_sentences(sents_doc))\n",
    "    start = 0\n",
    "    sents_bow = []\n",
    "    for d in sents_doc:\n",
    "        end = start + len(d)\n",
    "        sents_bow.append(X[start:end])\n",
    "        start = end\n",
    "\n",
    "#     sents_bow = [vct.transform(d) for d in sents_doc]\n",
    "    sents_lbl = [[l]*len(s) for l,s in  zip(data.target, sents_doc)]\n",
    "    \n",
    "    x = np.array([Document(a,b,c,d) for a,b,c,d in zip( sents_bow, sents_lbl, data.target)])\n",
    "    y = data.target\n",
    "    return x,y\n",
    "\n",
    "\n",
    "\n",
    "def get_context(doc, i):\n",
    "    ''' Get surrounding sentences for context '''\n",
    "    if i >= len(doc.sentences):\n",
    "        raise Exception(\"This doc is not that long.\")\n",
    "    if len(doc.sentences) == 1:\n",
    "        return np.array([])\n",
    "    if i==0: # for the first1\n",
    "        return np.array([doc.sent_bow[1]])\n",
    "    elif i == len(doc.sentences)-1: # for the last one \n",
    "        return np.array([doc.sent_bow[len(doc.sentences)-2]])\n",
    "    else: \n",
    "        return np.array([doc.sent_bow[i-1], doc.sent_bow[i+1]])\n",
    "\n",
    "def get_sentence_label(clf, x, threshold=.4):\n",
    "    '''Get a label or a neutral answer by uncertainty threshold.'''\n",
    "    \n",
    "    unc = 1-clf.predict_proba(x).max()\n",
    "    if unc < threshold :\n",
    "        return clf.predict(x)\n",
    "    else:\n",
    "        return 2 ## Neutral label class\n",
    "    \n",
    "def feature_context(doc, i, doc_clf, top=10, threshold=.47):\n",
    "    '''Feature function, context and lexicon counts, for one sentence '''\n",
    "    context = get_context(doc, i)\n",
    "    sent_lbl = get_sentence_label(doc_clf, doc.sent_bow[i], threshold=threshold)\n",
    "    lexicon = get_lexicon(doc_clf, top=top)\n",
    "    n_lex = len(lexicon)\n",
    "    n_feat = (3 * n_lex) + 1  # 2 context sentences and current sentence + label\n",
    "    lex_index = [x[0] for x in lexicon]\n",
    "\n",
    "    new_feat = np.zeros(n_feat)\n",
    "\n",
    "    # Add context sentences\n",
    "    for i,si in enumerate(context):\n",
    "        new_feat[i*n_lex:(i+1)*n_lex] =  si[0,lex_index].toarray()\n",
    "\n",
    "    #Add current sentence\n",
    "    new_feat[2*n_lex:3*n_lex] = doc.sent_bow[i][0,lex_index].toarray()\n",
    "    \n",
    "    # Add sentence label, predicted\n",
    "    # Last feature is the target label\n",
    "    new_feat[-1] = sent_lbl\n",
    "\n",
    "    return new_feat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def features_per_document(doc, doc_clf, feature_fn, top=10, threshold=.47):\n",
    "    x = np.vstack((feature_fn(doc, i, doc_clf, top=top, threshold=threshold) for i in range(len(doc.sentences))))\n",
    "    return x\n",
    "\n",
    "def get_training_sentence(documents, doc_clf, feature_fn, top=10, threshold=.47):\n",
    "    x = np.vstack((features_per_document(d, doc_clf, feature_fn, top=top, threshold=threshold) for d in documents))\n",
    "    return x[:,:-1], x[:,-1]\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x,y = load_documents(imdb.train, vct_doc, sent_tk)\n",
    "\n",
    "\n",
    "doc_clf = LogisticRegression(penalty='l1', C=1)\n",
    "doc_clf.fit(imdb.train.bow, imdb.train.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-doc accuracy: 0.8884\n",
      "Test size: 24989\n"
     ]
    }
   ],
   "source": [
    "# Testing document classifier\n",
    "\n",
    "print \"Document-doc accuracy: %.4f\" % metrics.accuracy_score(imdb.test.target, doc_clf.predict(imdb.test.bow))\n",
    "print \"Test size: %s\" % imdb.test.bow.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-sentence accuracy: 0.6503\n",
      "Test size: 304952\n"
     ]
    }
   ],
   "source": [
    "# Testing Document-sentence \n",
    "# Sentences take label of the document\n",
    "\n",
    "def doc_to_sents(docs_text, doc_labels, vct):\n",
    "    '''Create bow features for sentences of one document'''\n",
    "    \n",
    "    sent_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    doc_sents = sent_splitter.tokenize_sents(docs_text)\n",
    "    sizes = [len(d) for d in doc_sents]\n",
    "    labels = [[l]*s for l, s in zip(doc_labels, sizes)]\n",
    "    \n",
    "    sents_bow = vct.transform(iterate_sentences(doc_sents))\n",
    "    labels = np.array([l for l in iterate_sentences(labels)])\n",
    "    \n",
    "    return sents_bow, labels\n",
    "\n",
    "test_sx, test_sy = doc_to_sents(imdb.test.data, imdb.test.target, vct_doc)\n",
    "print \"Document-sentence accuracy: %.4f\" % metrics.accuracy_score(test_sy, doc_clf.predict(test_sx))\n",
    "print \"Test size: %s\" % len(test_sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent-doc accuracy: 0.9005\n",
      "Sent-sentence accuracy: 0.6982\n",
      "Test size: 304952\n"
     ]
    }
   ],
   "source": [
    "# Testing sentence to sentence\n",
    "train_sx, train_sy = doc_to_sents(imdb.train.data, imdb.train.target, vct_doc)\n",
    "s2s_clf = LogisticRegression(penalty='l1', C=1)\n",
    "s2s_clf.fit(train_sx, train_sy)\n",
    "print \"Sent-doc accuracy: %.4f\" % metrics.accuracy_score(imdb.test.target, s2s_clf.predict(imdb.test.bow))\n",
    "print \"Sent-sentence accuracy: %.4f\" % metrics.accuracy_score(test_sy, s2s_clf.predict(test_sx))\n",
    "print \"Test size: %s\" % len(test_sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perc. neutrals: 0.24120130037\n",
      "Accuracy: 0.717845485604\n"
     ]
    }
   ],
   "source": [
    "## Neutrality and accuracy of the oracle on sentences, on training data\n",
    "pred_prob = doc_clf.predict_proba(train_sx)\n",
    "pred_sent = doc_clf.predict(train_sx)\n",
    "unc_sent = 1- pred_prob.max(axis=1)\n",
    "thres = 0.42\n",
    "pred_sent[unc_sent > thres] = 2\n",
    "print \"Perc. neutrals: %s\" % (1. * len(pred_sent[unc_sent > thres]) / len(pred_sent))\n",
    "non_neu = pred_sent < 2\n",
    "print \"Accuracy: %s\" % (metrics.accuracy_score(np.array(train_sy)[non_neu], pred_sent[non_neu]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,  TransformerMixin,ClassifierMixin\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.scparse import vstack, hstack\n",
    "\n",
    "\n",
    "class ContextVectorizer(BaseEstimator, TransformerMixin):\n",
    "# class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for Vectorizer\"\"\"\n",
    "    def __init__(self, doc_clf, vct, feature_fn,top=10, threshold=.47):\n",
    "        self.doc_clf = doc_clf\n",
    "        self.vct = vct\n",
    "        self.feature_fn = feature_fn \n",
    "        self.top = top\n",
    "        self.threshold = threshold \n",
    "        self.feature_fn = self.feature_context\n",
    "        self.lexicon =  self.get_lexicon(self.doc_clf, top=self.top)\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        x = self.get_training_sentence(documents,  self.feature_fn, \n",
    "                                       threshold=self.threshold)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_lexicon(self, clf, top=10):\n",
    "        '''\n",
    "        Return lexicon of top K terms according to classifier clf. \n",
    "        The function returns feat_index-class pairs\n",
    "        '''\n",
    "    #     feats = np.array(vct.get_feature_names())\n",
    "        coefs = clf.coef_\n",
    "        if coefs.shape[0] == 1:\n",
    "            coefs = [-1 * coefs[0], coefs[0]]\n",
    "\n",
    "        res = []\n",
    "        for ci, cname in enumerate(clf.classes_): # for every class\n",
    "            coef = coefs[ci]\n",
    "            res.extend([(i, cname) for i in np.argsort(coef)[::-1][:top]])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def get_context(self, doc, i):\n",
    "        ''' Get surrounding sentences for context '''\n",
    "        zero_mat = csr_matrix(doc.sent_bow[0].shape, dtype=np.int8)\n",
    "        if i >= len(doc.sentences):\n",
    "            raise Exception(\"This doc is not that long.\")\n",
    "        if len(doc.sentences) == 1: # there is only one sentence\n",
    "            return np.array([zero_mat, doc.sent_bow[0], zero_mat])\n",
    "        elif i==0: # for the first1\n",
    "            return np.array([zero_mat, doc.sent_bow[0], doc.sent_bow[1]])\n",
    "        elif i == len(doc.sentences)-1: # for the last one \n",
    "            return np.array([doc.sent_bow[i-1], doc.sent_bow[i], zero_mat])\n",
    "        else: \n",
    "            return np.array([doc.sent_bow[i-1], doc.sent_bow[i], doc.sent_bow[i+1]])\n",
    "\n",
    "    def get_sentence_label(self,  x, threshold=.4):\n",
    "        '''Get a label or a neutral answer by uncertainty threshold.'''\n",
    "\n",
    "        unc = 1 - self.doc_clf.predict_proba(x).max()\n",
    "        if unc < threshold :\n",
    "            return self.doc_clf.predict(x)\n",
    "        else:\n",
    "            return 2 ## Neutral label class\n",
    "\n",
    "    def feature_context(self, doc, i,  threshold=.47):\n",
    "        '''Feature function, context and lexicon counts, for one sentence '''\n",
    "        context = self.get_context(doc, i)\n",
    "        sent_lbl = self.get_sentence_label(doc.sent_bow[i], threshold=threshold)\n",
    "\n",
    "        n_lex = len(self.lexicon)\n",
    "        n_feat = (3 * n_lex) + 1  # 2 context sentences and current sentence + label\n",
    "        lex_index = [x[0] for x in self.lexicon]\n",
    "\n",
    "        # Add context sentences + Add sentence label, predicted\n",
    "\n",
    "        new_feat =  hstack([c[:,lex_index] for c in context]+[csr_matrix(([sent_lbl],[0] , [0]), shape=(1, 1))])\n",
    "\n",
    "        if new_feat.shape[1] != n_feat:\n",
    "            raise ValueError(\"Oops, convertion did not work. The features are no correct.\")\n",
    "\n",
    "        return new_feat\n",
    "    \n",
    "    def iterate_sentences(self, documents):\n",
    "        for d in documents:\n",
    "            for s in d:\n",
    "                yield s\n",
    "\n",
    "    def features_per_document(self, doc,  feature_fn,  threshold=.47):\n",
    "        x = vstack((self.feature_fn(doc, i, threshold=threshold) for i in range(len(doc.sentences))))\n",
    "        return x\n",
    "\n",
    "    def get_training_sentence(self, documents,  feature_fn,  threshold=.47):\n",
    "        x = vstack((self.features_per_document(d, feature_fn,  threshold=threshold) for d in documents))\n",
    "        return x\n",
    "\n",
    "    def set_top(top):\n",
    "        self.top = top\n",
    "        \n",
    "    def set_unc_threshold(thr):\n",
    "        self.threshold = thr \n",
    "        \n",
    "        \n",
    "class SentenceClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Sentence Classifier. Takes data from ContextVectorizer\"\"\"\n",
    "    def __init__(self):\n",
    "        self.clf = LogisticRegression(penalty='l1', C=1)\n",
    "        \n",
    "        \n",
    "    def __init__(self, doc_clf, vct, feature_fn,top=10, threshold=.47):\n",
    "        self.clf = LogisticRegression(penalty='l1', C=1)\n",
    "        self.converter = ContextVectorizer(doc_clf, vct, feature_fn, top=top, threshold=threshold)\n",
    "        \n",
    "    def convert(self, x):\n",
    "        xx = self.converter.transform(x)\n",
    "        return xx\n",
    "    \n",
    "    def fit(self, x,y):\n",
    "        xx, yy = self.convert(x)\n",
    "        self.clf.fit(xx, yy)\n",
    "        self.classes_ = self.clf.classes_\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        xx = self.convert(X)\n",
    "\n",
    "        return self.clf.predict(xx)\n",
    "\n",
    "    def fit2(self, X, y):\n",
    "        self.clf.fit(X,y)\n",
    "        self.classes_ = self.clf.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict2(self, X):\n",
    "        return self.clf.predict(X)  \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        xx = self.convert(X)\n",
    "        return self.clf.predict_proba(xx)\n",
    "\n",
    "    def predict_proba2(self, X):\n",
    "        return self.clf.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24989\n"
     ]
    }
   ],
   "source": [
    "## Convert testing documents \n",
    "te_x, te_y = load_documents(imdb.test, vct_doc, sent_tk)\n",
    "print \"%s\" % (len(te_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "\n",
    "* create a training funtion for data with label from te document \n",
    "* create a function for data from amt with original labels (should work for other data as well)\n",
    "* create a fnction for testing same as training options \n",
    "* create a cv test \n",
    "* create a plot with cv\n",
    "* test base liens\n",
    "* test classifiers\n",
    "* test classifier with simple fieatures\n",
    "* with features for sentiment analysis \n",
    "* with fancy features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accu': 0.65625, 'test_size': 122, 'neutrals': 0.21311475409836067}\n"
     ]
    }
   ],
   "source": [
    "## Training with Documents\n",
    "\n",
    "def train_clf(x,y, clf, convert=False):\n",
    "    if convert:\n",
    "        clf.fit(x, y)\n",
    "    else:\n",
    "        clf.fit2(x,y)\n",
    "        \n",
    "    return clf\n",
    "\n",
    "def test_clf(x, clf, convert=False):\n",
    "    '''\n",
    "    Effective accuracy and neutrality percentage test\n",
    "    :param x: list of Document instances\n",
    "    '''\n",
    "    yy = []\n",
    "    for d in x:\n",
    "        yy.extend(d.sent_labels)\n",
    "    yy = np.array(yy)\n",
    "    \n",
    "    if convert:\n",
    "        pred = clf.predict(x)\n",
    "    else:\n",
    "        pred = clf.predict2(x)\n",
    "\n",
    "    non_neu = pred < 2\n",
    "    return {'accu':metrics.accuracy_score(yy[non_neu], pred[non_neu]), 'neutrals':1- 1.* sum(non_neu) / len(pred), 'test_size':len(pred)}\n",
    "\n",
    "print test_clf(x[:10],None, s, convert=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get learning curve data\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "\n",
    "def experiment(data, train_fn, test_fn, clf, train_sizes=np.linspace(.1,1.,5), n_folds=5, seed=12222):\n",
    "#     clf = SentenceClassifier(doc_clf, vct_doc, None, top=2500, threshold=.42)\n",
    "#     cv = cv.ShuffleSplit(len(y), n_iter=5, test_size=.0, random_state=12345)\n",
    "    cross_val = cv.KFold(len(data), n_folds=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    test_scores = []\n",
    "    \n",
    "    for train_index, test_index in cross_val:\n",
    "        trial_score = []\n",
    "        print \"Train size: %s, Test size: %s\" % (len(train_index), len(test_index))\n",
    "        test_x, test_y = clf.convert(x[test_index])\n",
    "        train_x, train_y = clf.convert(x[train_index[:max(train_sizes)]])\n",
    "        for size in train_sizes:\n",
    "            print \"Size: %s\" % size\n",
    "            print np.unique(train_y[:size])\n",
    "            trained = train_fn(train_x[:size], train_y[:size],clf, convert=False)\n",
    "            trial_score.append(test_fn(test_x, test_y, trained,convert=False))\n",
    "            print \"Accuracy: %s\" % trial_score[-1]\n",
    "        test_scores.append(trial_score)\n",
    "    \n",
    "    test_scores_mean = np.mean([t['accu'] for t in test_scores], axis=0)\n",
    "    test_scores_std = np.std([t['accu'] for t in test_scores], axis=0)\n",
    "    \n",
    "\n",
    "    return test_scores, test_scores_mean, test_scores_std \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exp_plot(results, sizes=None):\n",
    "    \n",
    "    for m, res in results.items():\n",
    "        avg = res[1]\n",
    "        std = res[2]\n",
    "        plt.plot(sizes,avg, label=m)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel('Accuracy of Student')\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 19992, Test size: 4999\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-0b54165c9185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m res1 = experiment(x, train_clf, test_clf, SentenceClassifier(doc_clf, vct_doc, None, top=2500, threshold=.42), \n\u001b[0;32m----> 4\u001b[0;31m                   train_sizes=sizes, n_folds=5, seed=12222)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-108-3d76e99a1e4a>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(data, train_fn, test_fn, clf, train_sizes, n_folds, seed)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrial_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Train size: %s, Test size: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-21160e3b4180>\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-21160e3b4180>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         x = self.get_training_sentence(documents,  self.feature_fn, \n\u001b[0;32m---> 20\u001b[0;31m                                        threshold=self.threshold)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-21160e3b4180>\u001b[0m in \u001b[0;36mget_training_sentence\u001b[0;34m(self, documents, feature_fn, threshold)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_training_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfeature_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.47\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_per_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#         return x[:,:-1], x[:,-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-21160e3b4180>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((d,))\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_training_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfeature_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.47\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_per_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#         return x[:,:-1], x[:,-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-21160e3b4180>\u001b[0m in \u001b[0;36mfeatures_per_document\u001b[0;34m(self, doc, feature_fn, threshold)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeatures_per_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfeature_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.47\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-21160e3b4180>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((i,))\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeatures_per_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfeature_fn\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.47\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-21160e3b4180>\u001b[0m in \u001b[0;36mfeature_context\u001b[0;34m(self, doc, i, threshold)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Add context sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mnew_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_lex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_lex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlex_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m#Add current sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/csr.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;31m# [i, [1, 2]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0missequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/csr.pyc\u001b[0m in \u001b[0;36mextractor\u001b[0;34m(indices, N)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mslicing\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mform\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mmin_indx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/csr.pyc\u001b[0m in \u001b[0;36masindices\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    224\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index"
     ]
    }
   ],
   "source": [
    "sizes = [100, 250, 500, 1000, 2000, 3000]\n",
    "sizes = [10, 20]\n",
    "res1 = experiment(x, train_clf, test_clf, SentenceClassifier(doc_clf, vct_doc, None, top=2500, threshold=.42), \n",
    "                  train_sizes=sizes, n_folds=5, seed=12222)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "s.convert(x[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_plot(res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Feature Spaces \n",
    "\n",
    "## 1. Sentiment-based Features\n",
    "\n",
    "Based on (McDonald, 2011): \n",
    "\n",
    "Features are s :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train sentence classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "context_vect = ContextVectorizer(doc_clf, vct_doc, feature_context ,top=2500, threshold=.45)\n",
    "sent_cla = SentenceClassifier()\n",
    "\n",
    "## Get the training data\n",
    "# Get all and train classifier, fully trained\n",
    "ss2s_clf = Pipeline(steps=[('context', context_vect),('estimator', sent_cla)])\n",
    "ss2s_clf.fit(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Get the test data\n",
    "# Transform all test\n",
    "text_x,test_y = load_documents(imdb.test, vct_doc, sent_tk)\n",
    "testx_ss2s = context_vect.transform(test_x)\n",
    "\n",
    "# print \"Sent-doc accuracy: %.4f\" % metrics.accuracy_score(testx_ss2s[:,-1], s2s_clf.predict(testx_ss2s[:,:-1]))\n",
    "print \"Sent-sentence accuracy: %.4f\" % metrics.accuracy_score(testx_ss2s[:,-1], s2s_clf.predict(testx_ss2s[:,:-1]))\n",
    "print \"Test size: %s\" % (testx_ss2s.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.570829840738\n",
      "0.570829840738\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.utils import resample\n",
    "# >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n",
    "\n",
    "sent_x\n",
    "\n",
    "# # prueba =  context_vect.transform(x[:10])\n",
    "# # print prueba[:,-1]\n",
    "\n",
    "\n",
    "# # prueba2 = get_training_sentence(x[:10], doc_clf, feature_context, top=10, threshold=.47)\n",
    "# # print prueba2[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sent_cla = SentenceClassifier()\n",
    "clf_sent = Pipeline(steps=[('context', context_vect),('estimator', sent_cla)])\n",
    "\n",
    "# clf_sent.fit(sub_x, sub_y)\n",
    "\n",
    "# t_x = context_vect.transform(sub_x)\n",
    "print metrics.accuracy_score(t_x[:,-1], sent_cla.predict(t_x))\n",
    "print metrics.accuracy_score(t_x[:,-1], clf_sent.predict(sub_x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., ...,  0.,  1.],\n",
       "       [ 0.,  0., ...,  0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train on bootstraps, test on amt data sentences\n",
    "#_# train on bootstraps, test on sentences as documents \n",
    "t_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193, 61)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on AMT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "from utilities.amt_datautils import load_amt_imdb\n",
    "amt = load_amt_imdb(IMDB_DATA, shuffle=True, rnd=1928374, amt_labels='labels')  # should bring with training labels as the amt annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n"
     ]
    }
   ],
   "source": [
    "def load_documents_amt(data, vct, sent_tk ):\n",
    "    # sents, sents_bow, sents_lbl, doc_lbl\n",
    "    \n",
    "    sents_doc = [d.split('THIS_IS_A_SEPARATOR') for d in data.data]\n",
    "    sents_bow = [vct.transform(d) for d in sents_doc]\n",
    "    sents_lbl = data.target\n",
    "    \n",
    "    x = np.array([Document(a,b,c,d) for a,b,c,d in zip(sents_doc, sents_bow, sents_lbl, data.doctarget)])\n",
    "    y = data.doctarget\n",
    "    return x,y\n",
    "\n",
    "\n",
    "print amt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['docid', 'target', 'DESCR', 'alltarget', 'doctarget', 'filenames', 'target_names', 'data', 'alldata']\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "print amt.train.keys()\n",
    "print len(amt.train.doctarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert document to new feature space\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import DictVectorizer\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     row = [0] * n_features\n",
    "#     col = [l[0] for l in lexicon]\n",
    "#     counts = d.sent_bow[:,col]\n",
    "    \n",
    "#     data= counts.sum(axis=0)\n",
    "#     return csr_matrix( (data,(row,col)), shape=(1,n_features) )\n",
    "\n",
    "#     csr_matrix()\n",
    "\n",
    "def feature_simple_counts(doc, i, doc_clf):\n",
    "    '''Feature function bag of words, no context'''\n",
    "    return doc.sent_bow[i]\n",
    "    \n",
    "def featurize(documents, labels, clf_d, feature_fn):\n",
    "    '''Create a feature vector from documents'''\n",
    "\n",
    "    lexicon = get_lexicon(clf_d, top=10) \n",
    "    x = vstack((feature_fn(d, lexicon, clf_d) for d in documents))\n",
    "    return x\n",
    "    \n",
    "def feature_counts(doc, lexicon, clf_d):\n",
    "    n_features = len(lexicon) * 4 + 1\n",
    "    row = [0] * n_features\n",
    "    col = [l[0] for l in lexicon]\n",
    "    counts = d.sent_bow[:,col]\n",
    "    \n",
    "    data= counts.sum(axis=0)\n",
    "    return csr_matrix( (data,(row,col)), shape=(1,n_features) )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "1. Load train and test\n",
    "1. Features for train:\n",
    "    1. for documents: vectorizer like before\n",
    "    1. for sentences: lexicon counts\n",
    "1. features for test: same as for sentences (this is amt data)\n",
    "1. for every size of the bootstrap\n",
    "    1. train a document classifier\n",
    "    1. obtain sentences and featurize\n",
    "    1. test document\n",
    "    1. test sentence\n",
    "    1. save results\n",
    "    \n",
    "### Features per sentence\n",
    "\n",
    "1. For every document\n",
    "    1. for every sentence\n",
    "        1. get context, document, and label, and lexicon\n",
    "        1. build a vector\n",
    "        1. return vecotr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def experiment(data, vct, runs, rnd=123):\n",
    "    x_doc, y_doc = load_documents(data, vct)\n",
    "    for train, test in cv:\n",
    "        clf_d = LogisticRegression(penalty=\"l1\", C=1)\n",
    "        clf_s = LogisticRegression(penalty=\"l1\", C=1)\n",
    "        \n",
    "        clf_d.fit(data.train.bow[train], data.train.target[train])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_predict():\n",
    "    pass\n",
    "def lr_fit():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
