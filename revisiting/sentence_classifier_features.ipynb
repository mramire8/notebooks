{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classifier with Feature Template\n",
    "\n",
    "## Settings\n",
    "\n",
    "### Feature Function\n",
    "The feature function $\\mathbf{f}$ uses the following features to represent a sentence: \n",
    "\n",
    "* Features for sentence context, each sentnece and its neighbors will have the following:\n",
    "    * Number of tokens\n",
    "    * Number of positive, negative and neutral tokens\n",
    "    * Proportion of positive over negative\n",
    "    * Proportion of negative over postive\n",
    "    * Proportion of neutral \n",
    "* Feature for document context:\n",
    "    * Same as the sentences but for the full document\n",
    "* Labels\n",
    "    * Sentence level label from DaS classifier ($y_i^s$)\n",
    "    \n",
    "\n",
    "### Training the Classifier\n",
    "\n",
    "We select N random documents and train a DaS classifier (trained on documents) to predict the label of the sentence $y^s_i$. We create a logistic regression classifier that will be trained on data using the feature fucntion representation $P_E(y^s|\\mathbf{f}(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Imports \n",
    "%matplotlib inline\n",
    "\n",
    "STRUCTURED = '/Users/maru/MyCode/structured'\n",
    "IMDB_DATA='/Users/maru/MyCode/data/imdb'\n",
    "SRAA_DATA='/Users/maru/MyCode/data/sraa'\n",
    "TWIITER_DATA = '/Users/maru/MyCode/data/twitter'\n",
    "\n",
    "# STRUCTURED = '/Users/maru/My Code/structured'\n",
    "# IMDB_DATA='/Users/maru/Dataset/aclImdb'\n",
    "# SRAA_DATA='/Users/maru/Dataset/aviation/data'\n",
    "# TWIITER_DATA = '/Users/maru/Dataset/twitter'\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(STRUCTURED))\n",
    "sys.path.append(os.path.abspath('C:/cygwin/home/mramire8/python_code/structured/'))\n",
    "\n",
    "import learner\n",
    "\n",
    "from utilities.datautils import load_dataset\n",
    "import experiment.base as exp\n",
    "\n",
    "\n",
    "import utilities.experimentutils as exputil\n",
    "import utilities.datautils as datautil\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get the data ready\n",
    "import re \n",
    "vct = CountVectorizer(min_df=2, token_pattern=re.compile(r'(?u)\\b\\w+\\b'))\n",
    "\n",
    "\n",
    "sent_tk = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "imdb =  load_dataset(\"imdb\",IMDB_DATA, keep_subject=True)\n",
    "\n",
    "imdb.train.bow = vct_doc.fit_transform(imdb.train.data)\n",
    "imdb.test.bow = vct_doc.transform(imdb.test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document(object):\n",
    "    def __init__(self, raw_text, lbl, sent_tk, vct_gral, sent_lbl=None):\n",
    "        self.sentences = sent_tk.tokenize_sents([raw_text])[0]\n",
    "        self.doc_label = lbl\n",
    "        self.sent_bow = vct_gral.transform(self.sentences) # counts per sentence\n",
    "        if sent_lbl is not None:\n",
    "            self.sent_labels = [lbl] * len(self.sentences)\n",
    "        else:\n",
    "            self.sent_labels = sent_lbl#np.array([s.split('\\t')[0] for s in self.sentences])\n",
    "            \n",
    "    def __init__(self, sents, sents_bow, sents_lbl, doc_lbl):\n",
    "        self.sentences = sents\n",
    "        self.doc_label = doc_lbl\n",
    "        self.sent_labels = sents_lbl\n",
    "        self.sent_bow = sents_bow\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_lexicon(clf, top=10):\n",
    "    '''\n",
    "    Return lexicon of top K terms according to classifier clf. \n",
    "    The function returns feat_index-class pairs\n",
    "    '''\n",
    "#     feats = np.array(vct.get_feature_names())\n",
    "    coefs = clf.coef_\n",
    "    if coefs.shape[0] == 1:\n",
    "        coefs = [-1 * coefs[0], coefs[0]]\n",
    "        \n",
    "    res = []\n",
    "    for ci, cname in enumerate(clf.classes_): # for every class\n",
    "        coef = coefs[ci]\n",
    "        res.extend([(i, cname) for i in np.argsort(coef)[::-1][:top]])\n",
    "    return res\n",
    " \n",
    "    \n",
    "def load_documents(data, vct, sent_tk ):\n",
    "    # sents, sents_bow, sents_lbl, doc_lbl\n",
    "    \n",
    "    sents_doc = sent_tk.tokenize_sents(data.data)\n",
    "    sents_bow = [vct.transform(d) for d in sents_doc]\n",
    "    sents_lbl = [[l]*len(s) for l,s in  zip(data.target, sents_doc)]\n",
    "    \n",
    "    x = np.array([Document(a,b,c,d) for a,b,c,d in zip(sents_doc, sents_bow, sents_lbl, data.target)])\n",
    "    y = data.target\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def get_context(doc, i):\n",
    "    ''' Get surrounding sentences for context '''\n",
    "    if i >= len(doc.sentences):\n",
    "        raise Exception(\"This doc is not that long.\")\n",
    "    if len(doc.sentences) == 1:\n",
    "        return np.array([])\n",
    "    if i==0: # for the first1\n",
    "        return np.array([doc.sent_bow[1]])\n",
    "    elif i == len(doc.sentences)-1: # for the last one \n",
    "        return np.array([doc.sent_bow[len(doc.sentences)-2]])\n",
    "    else: \n",
    "        return np.array([doc.sent_bow[i-1], doc.sent_bow[i+1]])\n",
    "\n",
    "def get_sentence_label(clf, x, threshold=.4):\n",
    "    '''Get a label or a neutral answer by uncertainty threshold.'''\n",
    "    \n",
    "    unc = 1-clf.predict_proba(x).max()\n",
    "    if unc < threshold :\n",
    "        return clf.predict(x)\n",
    "    else:\n",
    "        return 2 ## Neutral label class\n",
    "    \n",
    "def feature_context(doc, i, doc_clf, top=10, threshold=.47):\n",
    "    '''Feature function, context and lexicon counts, for one sentence '''\n",
    "    context = get_context(doc, i)\n",
    "    sent_lbl = get_sentence_label(doc_clf, doc.sent_bow[i], threshold=threshold)\n",
    "    lexicon = get_lexicon(doc_clf, top=top)\n",
    "    n_lex = len(lexicon)\n",
    "    n_feat = (3 * n_lex) + 1  # 2 context sentences and current sentence + label\n",
    "    lex_index = [x[0] for x in lexicon]\n",
    "\n",
    "    new_feat = np.zeros(n_feat)\n",
    "\n",
    "    # Add context sentences\n",
    "    for i,si in enumerate(context):\n",
    "        new_feat[i*n_lex:(i+1)*n_lex] =  si[0,lex_index].toarray()\n",
    "\n",
    "    #Add current sentence\n",
    "    new_feat[2*n_lex:3*n_lex] = doc.sent_bow[i][0,lex_index].toarray()\n",
    "    \n",
    "    # Add sentence label, predicted\n",
    "    # Last feature is the target label\n",
    "    new_feat[-1] = sent_lbl\n",
    "\n",
    "    return new_feat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_sentences(documents):\n",
    "    for d in documents:\n",
    "        for s in d:\n",
    "            yield s\n",
    "\n",
    "def features_per_document(doc, doc_clf, feature_fn, top=10, threshold=.47):\n",
    "    x = np.vstack((feature_fn(doc, i, doc_clf, top=top, threshold=threshold) for i in range(len(doc.sentences))))\n",
    "    return x\n",
    "\n",
    "def get_training_sentence(documents, doc_clf, feature_fn, top=10, threshold=.47):\n",
    "    x = np.vstack((features_per_document(d, doc_clf, feature_fn, top=top, threshold=threshold) for d in documents))\n",
    "    return x[:,:-1], x[:,-1]\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vct_doc = CountVectorizer(encoding='ISO-8859-1', min_df=2, max_df=1.0, binary=True, token_pattern='\\\\b\\\\w+\\\\b')\n",
    "vct_doc = exputil.get_vectorizer({'vectorizer':'bow', 'limit':None, 'min_size':2})\n",
    "\n",
    "imdb.train.bow = vct_doc.fit_transform(imdb.train.data)\n",
    "imdb.test.bow = vct_doc.transform(imdb.test.data)\n",
    "\n",
    "x,y = load_documents(imdb.train, vct_doc, sent_tk)\n",
    "\n",
    "\n",
    "doc_clf = LogisticRegression(penalty='l1', C=1)\n",
    "doc_clf.fit(imdb.train.bow, imdb.train.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-doc accuracy: 0.8884\n",
      "Test size: 24989\n"
     ]
    }
   ],
   "source": [
    "# Testing document classifier\n",
    "\n",
    "print \"Document-doc accuracy: %.4f\" % metrics.accuracy_score(imdb.test.target, doc_clf.predict(imdb.test.bow))\n",
    "print \"Test size: %s\" % imdb.test.bow.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-sentence accuracy: 0.6509\n",
      "Test size: 304952\n"
     ]
    }
   ],
   "source": [
    "# Testing Document-sentence \n",
    "# Sentences take label of the document\n",
    "\n",
    "def doc_to_sents(docs_text, doc_labels, vct):\n",
    "    '''Create bow features for sentences of one document'''\n",
    "    \n",
    "    sent_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    doc_sents = sent_splitter.tokenize_sents(docs_text)\n",
    "    sizes = [len(d) for d in doc_sents]\n",
    "    labels = [[l]*s for l, s in zip(doc_labels, sizes)]\n",
    "    \n",
    "    sents_bow = vct.transform(iterate_sentences(doc_sents))\n",
    "    labels = [l for l in iterate_sentences(labels)]\n",
    "    \n",
    "    return sents_bow, labels\n",
    "\n",
    "test_sx, test_sy = doc_to_sents(imdb.test.data, imdb.test.target, vct_doc)\n",
    "print \"Document-sentence accuracy: %.4f\" % metrics.accuracy_score(test_sy, doc_clf.predict(test_sx))\n",
    "print \"Test size: %s\" % len(test_sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent-doc accuracy: 0.9004\n",
      "Sent-sentence accuracy: 0.6982\n",
      "Test size: 304952\n"
     ]
    }
   ],
   "source": [
    "# Testing sentence to sentence\n",
    "train_sx, train_sy = doc_to_sents(imdb.train.data, imdb.train.target, vct_doc)\n",
    "s2s_clf = LogisticRegression(penalty='l1', C=1)\n",
    "s2s_clf.fit(train_sx, train_sy)\n",
    "print \"Sent-doc accuracy: %.4f\" % metrics.accuracy_score(imdb.test.target, s2s_clf.predict(imdb.test.bow))\n",
    "print \"Sent-sentence accuracy: %.4f\" % metrics.accuracy_score(test_sy, s2s_clf.predict(test_sx))\n",
    "print \"Test size: %s\" % len(test_sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,  TransformerMixin,ClassifierMixin\n",
    "\n",
    "class ContextVectorizer(BaseEstimator, TransformerMixin):\n",
    "# class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for Vectorizer\"\"\"\n",
    "    def __init__(self, doc_clf, vct, feature_fn,top=10, threshold=.47):\n",
    "        self.doc_clf = doc_clf\n",
    "        self.vct = vct\n",
    "        self.feature_fn = feature_fn \n",
    "        self.top = top\n",
    "        self.threshold = threshold \n",
    "        self.feature_fn = self.feature_context\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        x = self.get_training_sentence(documents, self.doc_clf, self.feature_fn, \n",
    "                                       top=self.top, threshold=self.threshold)\n",
    "        return x\n",
    "\n",
    "    def get_lexicon(self, clf, top=10):\n",
    "        '''\n",
    "        Return lexicon of top K terms according to classifier clf. \n",
    "        The function returns feat_index-class pairs\n",
    "        '''\n",
    "    #     feats = np.array(vct.get_feature_names())\n",
    "        coefs = clf.coef_\n",
    "        if coefs.shape[0] == 1:\n",
    "            coefs = [-1 * coefs[0], coefs[0]]\n",
    "\n",
    "        res = []\n",
    "        for ci, cname in enumerate(clf.classes_): # for every class\n",
    "            coef = coefs[ci]\n",
    "            res.extend([(i, cname) for i in np.argsort(coef)[::-1][:top]])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def get_context(self, doc, i):\n",
    "        ''' Get surrounding sentences for context '''\n",
    "        if i >= len(doc.sentences):\n",
    "            raise Exception(\"This doc is not that long.\")\n",
    "        if len(doc.sentences) == 1:\n",
    "            return np.array([])\n",
    "        if i==0: # for the first1\n",
    "            return np.array([doc.sent_bow[1]])\n",
    "        elif i == len(doc.sentences)-1: # for the last one \n",
    "            return np.array([doc.sent_bow[len(doc.sentences)-2]])\n",
    "        else: \n",
    "            return np.array([doc.sent_bow[i-1], doc.sent_bow[i+1]])\n",
    "\n",
    "    def get_sentence_label(self, clf, x, threshold=.4):\n",
    "        '''Get a label or a neutral answer by uncertainty threshold.'''\n",
    "\n",
    "        unc = 1-clf.predict_proba(x).max()\n",
    "        if unc < threshold :\n",
    "            return clf.predict(x)\n",
    "        else:\n",
    "            return 2 ## Neutral label class\n",
    "\n",
    "    def feature_context(self, doc, i, doc_clf, top=10, threshold=.47):\n",
    "        '''Feature function, context and lexicon counts, for one sentence '''\n",
    "        context = self.get_context(doc, i)\n",
    "        sent_lbl = self.get_sentence_label(doc_clf, doc.sent_bow[i], threshold=threshold)\n",
    "        lexicon = self.get_lexicon(doc_clf, top=top)\n",
    "        n_lex = len(lexicon)\n",
    "        n_feat = (3 * n_lex) + 1  # 2 context sentences and current sentence + label\n",
    "        lex_index = [x[0] for x in lexicon]\n",
    "\n",
    "        new_feat = np.zeros(n_feat)\n",
    "\n",
    "        # Add context sentences\n",
    "        for i,si in enumerate(context):\n",
    "            new_feat[i*n_lex:(i+1)*n_lex] =  si[0,lex_index].toarray()\n",
    "\n",
    "        #Add current sentence\n",
    "        new_feat[2*n_lex:3*n_lex] = doc.sent_bow[i][0,lex_index].toarray()\n",
    "\n",
    "        # Add sentence label, predicted\n",
    "        # Last feature is the target label\n",
    "        new_feat[-1] = sent_lbl\n",
    "\n",
    "        return new_feat\n",
    "    \n",
    "    def iterate_sentences(self, documents):\n",
    "        for d in documents:\n",
    "            for s in d:\n",
    "                yield s\n",
    "\n",
    "    def features_per_document(self, doc, doc_clf, feature_fn, top=10, threshold=.47):\n",
    "        x = np.vstack((self.feature_fn(doc, i, doc_clf, top=top, threshold=threshold) for i in range(len(doc.sentences))))\n",
    "        return x\n",
    "\n",
    "    def get_training_sentence(self, documents, doc_clf, feature_fn, top=10, threshold=.47):\n",
    "        x = np.vstack((self.features_per_document(d, doc_clf, feature_fn, top=top, threshold=threshold) for d in documents))\n",
    "        return x\n",
    "#         return x[:,:-1], x[:,-1]\n",
    "    def set_top(top):\n",
    "        self.top = top\n",
    "    def set_unc_threshold(thr):\n",
    "        self.threshold = t\u001f\u001f\u001f \u001f\u001fhr\n",
    "\n",
    "class SentenceClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Sentence Classifier. Takes data from ContextVectorizer\"\"\"\n",
    "    def __init__(self):\n",
    "        self.clf = LogisticRegression(penalty='l1', C=1)\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf.fit(X[:,:-1],X[:,-1])\n",
    "        self.classes_ = self.clf.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X[:,:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train sentence classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "context_vect = ContextVectorizer(doc_clf, vct_doc, feature_context ,top=2500, threshold=.45)\n",
    "sent_cla = SentenceClassifier()\n",
    "\n",
    "## Get the training data\n",
    "# Get all and train classifier, fully trained\n",
    "ss2s_clf = Pipeline(steps=[('context', context_vect),('estimator', sent_cla)])\n",
    "ss2s_clf.fit(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Get the test data\n",
    "# Transform all test\n",
    "text_x,test_y = load_documents(imdb.test, vct_doc, sent_tk)\n",
    "testx_ss2s = context_vect.transform(test_x)\n",
    "\n",
    "# print \"Sent-doc accuracy: %.4f\" % metrics.accuracy_score(testx_ss2s[:,-1], s2s_clf.predict(testx_ss2s[:,:-1]))\n",
    "print \"Sent-sentence accuracy: %.4f\" % metrics.accuracy_score(testx_ss2s[:,-1], s2s_clf.predict(testx_ss2s[:,:-1]))\n",
    "print \"Test size: %s\" % (testx_ss2s.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.570829840738\n",
      "0.570829840738\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.utils import resample\n",
    "# >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n",
    "\n",
    "sent_x\n",
    "\n",
    "# # prueba =  context_vect.transform(x[:10])\n",
    "# # print prueba[:,-1]\n",
    "\n",
    "\n",
    "# # prueba2 = get_training_sentence(x[:10], doc_clf, feature_context, top=10, threshold=.47)\n",
    "# # print prueba2[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sent_cla = SentenceClassifier()\n",
    "clf_sent = Pipeline(steps=[('context', context_vect),('estimator', sent_cla)])\n",
    "\n",
    "# clf_sent.fit(sub_x, sub_y)\n",
    "\n",
    "# t_x = context_vect.transform(sub_x)\n",
    "print metrics.accuracy_score(t_x[:,-1], sent_cla.predict(t_x))\n",
    "print metrics.accuracy_score(t_x[:,-1], clf_sent.predict(sub_x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., ...,  0.,  1.],\n",
       "       [ 0.,  0., ...,  0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  0.]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train on bootstraps, test on amt data sentences\n",
    "#_# train on bootstraps, test on sentences as documents \n",
    "t_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193, 61)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Learning curve \n",
    "## Get learning curve data\n",
    "from sklearn import cross_validation\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "def get_learning_curve(estimator, X, y, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    return train_sizes, test_scores_mean, test_scores_std\n",
    "\n",
    "def cv_estimator(estimator, sx, sy, n_folds, seed=123):\n",
    "    cv = cross_validation.KFold(len(sx), n_folds=3, random_state=seed)\n",
    "    results = []\n",
    "    for train, test in cv:\n",
    "        estimator.fit(sx[train], sy[train])\n",
    "        results.append(metrics.accuracy_score(sx[test][:,-1], estimator.predict(sx[test])))\n",
    "    return np.mean(results), np.std(results)\n",
    "\n",
    "results = []\n",
    "for n in [100, 200, 300]:\n",
    "    results.append(cv_estimator(clf_sent, np.array(x[:n]),y[:n], 3, seed=1123))\n",
    "\n",
    "plt.plot([100, 200, 300], results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "from utilities.amt_datautils import load_amt_imdb\n",
    "amt = load_amt_imdb(IMDB_DATA, shuffle=True, rnd=1928374, amt_labels='labels')  # should bring with training labels as the amt annotations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert document to new feature space\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import DictVectorizer\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     row = [0] * n_features\n",
    "#     col = [l[0] for l in lexicon]\n",
    "#     counts = d.sent_bow[:,col]\n",
    "    \n",
    "#     data= counts.sum(axis=0)\n",
    "#     return csr_matrix( (data,(row,col)), shape=(1,n_features) )\n",
    "\n",
    "#     csr_matrix()\n",
    "\n",
    "def feature_simple_counts(doc, i, doc_clf):\n",
    "    '''Feature function bag of words, no context'''\n",
    "    return doc.sent_bow[i]\n",
    "    \n",
    "def featurize(documents, labels, clf_d, feature_fn):\n",
    "    '''Create a feature vector from documents'''\n",
    "\n",
    "    lexicon = get_lexicon(clf_d, top=10) \n",
    "    x = vstack((feature_fn(d, lexicon, clf_d) for d in documents))\n",
    "    return x\n",
    "    \n",
    "def feature_counts(doc, lexicon, clf_d):\n",
    "    n_features = len(lexicon) * 4 + 1\n",
    "    row = [0] * n_features\n",
    "    col = [l[0] for l in lexicon]\n",
    "    counts = d.sent_bow[:,col]\n",
    "    \n",
    "    data= counts.sum(axis=0)\n",
    "    return csr_matrix( (data,(row,col)), shape=(1,n_features) )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "1. Load train and test\n",
    "1. Features for train:\n",
    "    1. for documents: vectorizer like before\n",
    "    1. for sentences: lexicon counts\n",
    "1. features for test: same as for sentences (this is amt data)\n",
    "1. for every size of the bootstrap\n",
    "    1. train a document classifier\n",
    "    1. obtain sentences and featurize\n",
    "    1. test document\n",
    "    1. test sentence\n",
    "    1. save results\n",
    "    \n",
    "### Features per sentence\n",
    "\n",
    "1. For every document\n",
    "    1. for every sentence\n",
    "        1. get context, document, and label, and lexicon\n",
    "        1. build a vector\n",
    "        1. return vecotr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def experiment(data, vct, runs, rnd=123):\n",
    "    x_doc, y_doc = load_documents(data, vct)\n",
    "    for train, test in cv:\n",
    "        clf_d = LogisticRegression(penalty=\"l1\", C=1)\n",
    "        clf_s = LogisticRegression(penalty=\"l1\", C=1)\n",
    "        \n",
    "        clf_d.fit(data.train.bow[train], data.train.target[train])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_predict():\n",
    "    pass\n",
    "def lr_fit():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
