{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classifier with Feature Template\n",
    "\n",
    "## Settings\n",
    "\n",
    "### Feature Function\n",
    "The feature function $\\mathbf{f}$ uses the following features to represent a sentence: \n",
    "\n",
    "* Features for sentence context, each sentnece and its neighbors will have the following:\n",
    "    * Number of tokens\n",
    "    * Number of positive, negative and neutral tokens\n",
    "    * Proportion of positive over negative\n",
    "    * Proportion of negative over postive\n",
    "    * Proportion of neutral \n",
    "* Feature for document context:\n",
    "    * Same as the sentences but for the full document\n",
    "* Labels\n",
    "    * Sentence level label from DaS classifier ($y_i^s$)\n",
    "    \n",
    "\n",
    "### Training the Classifier\n",
    "\n",
    "We select N random documents and train a DaS classifier (trained on documents) to predict the label of the sentence $y^s_i$. We create a logistic regression classifier that will be trained on data using the feature fucntion representation $P_E(y^s|\\mathbf{f}(x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Imports \n",
    "%matplotlib inline\n",
    "\n",
    "STRUCTURED = '/Users/maru/MyCode/structured'\n",
    "IMDB_DATA='/Users/maru/MyCode/data/imdb'\n",
    "SRAA_DATA='/Users/maru/MyCode/data/sraa'\n",
    "TWIITER_DATA = '/Users/maru/MyCode/data/twitter'\n",
    "\n",
    "# IMDB_DATA = 'C:/Users/mramire8/Documents/Research/Oracle confidence and Interruption/dataset/aclImdb/raw-data'\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(STRUCTURED))\n",
    "sys.path.append(os.path.abspath('C:/cygwin/home/mramire8/python_code/structured/'))\n",
    "\n",
    "import learner\n",
    "\n",
    "from utilities.datautils import load_dataset\n",
    "import experiment.base as exp\n",
    "\n",
    "\n",
    "import utilities.experimentutils as exputil\n",
    "import utilities.datautils as datautil\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get the data ready\n",
    "import re \n",
    "vct = CountVectorizer(min_df=2, token_pattern=re.compile(r'(?u)\\b\\w+\\b'))\n",
    "\n",
    "# vct_doc = CountVectorizer(encoding='ISO-8859-1', min_df=2, max_df=1.0, binary=True, token_pattern='\\\\b\\\\w+\\\\b')\n",
    "vct_doc = exputil.get_vectorizer({'vectorizer':'bow', 'limit':None, 'min_size':2})\n",
    "\n",
    "IMDB_DATA = 'C:/Users/mramire8/Documents/Research/Oracle confidence and Interruption/dataset/aclImdb/raw-data'\n",
    "\n",
    "sent_tk = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "imdb =  load_dataset(\"imdb\",IMDB_DATA, keep_subject=True)\n",
    "\n",
    "imdb.train.bow = vct_doc.fit_transform(imdb.train.data)\n",
    "imdb.test.bow = vct_doc.transform(imdb.test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document(object):\n",
    "    def __init__(self, raw_text, lbl, sent_tk, vct_gral, sent_lbl=None):\n",
    "        sentences = sent_tk.tokenize_sents([raw_text])[0]\n",
    "        self.doc_label = lbl\n",
    "        self.sent_bow = vct_gral.transform(sentences) # counts per sentence\n",
    "        if sent_lbl is not None:\n",
    "            self.sent_labels = [lbl] * len(sentences)\n",
    "        else:\n",
    "            self.sent_labels = sent_lbl#np.array([s.split('\\t')[0] for s in self.sentences])\n",
    "            \n",
    "    def __init__(self,  sents_bow, sents_lbl, doc_lbl):\n",
    "#         self.sentences = sents\n",
    "        self.doc_label = doc_lbl\n",
    "        self.sent_labels = sents_lbl\n",
    "        self.sent_bow = sents_bow\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_sentences(documents):\n",
    "    for d in documents:\n",
    "        for s in d:\n",
    "            yield s\n",
    " \n",
    "    \n",
    "def load_documents(data, vct, sent_tk ):\n",
    "    # sents, sents_bow, sents_lbl, doc_lbl\n",
    "    \n",
    "    sents_doc = sent_tk.tokenize_sents(data.data)\n",
    "    sents_bow = [vct.transform(d) for d in sents_doc]\n",
    "    sents_lbl = [[l]*len(s) for l,s in  zip(data.target, sents_doc)]\n",
    "    \n",
    "    x = np.array([Document(a,b,c) for a,b,c in zip( sents_bow, sents_lbl, data.target)])\n",
    "    y = data.target\n",
    "    return x,y\n",
    "\n",
    "def load_documents_v2(data, vct, sent_tk, doc_clf ):\n",
    "    # sents, sents_bow, sents_lbl, doc_lbl\n",
    "    \n",
    "    sents_doc = sent_tk.tokenize_sents(data.data)\n",
    "\n",
    "    X = vec.fit_transform(iterate_sentences(sents_doc))\n",
    "    start = 0\n",
    "    sents_bow = []\n",
    "    for d in sents_doc:\n",
    "        end = start + len(d)\n",
    "        sents_bow.append(X[start:end])\n",
    "        start = end\n",
    "\n",
    "#     sents_bow = [vct.transform(d) for d in sents_doc]\n",
    "    sents_lbl = [[l]*len(s) for l,s in  zip(data.target, sents_doc)]\n",
    "    \n",
    "    x = np.array([Document(a,b,c) for a,b,c in zip( sents_bow, sents_lbl, data.target)])\n",
    "    y = data.target\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def features_per_document(doc, doc_clf, feature_fn, top=10, threshold=.47):\n",
    "    x = np.vstack((feature_fn(doc, i, doc_clf, top=top, threshold=threshold) for i in range(len(doc.sent_labels))))\n",
    "    return x\n",
    "\n",
    "def get_training_sentence(documents, doc_clf, feature_fn, top=10, threshold=.47):\n",
    "    x = np.vstack((features_per_document(d, doc_clf, feature_fn, top=top, threshold=threshold) for d in documents))\n",
    "    return x[:,:-1], x[:,-1]\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x,y = load_documents(imdb.train, vct_doc, sent_tk)\n",
    "\n",
    "\n",
    "doc_clf = LogisticRegression(penalty='l1', C=1)\n",
    "doc_clf.fit(imdb.train.bow, imdb.train.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-doc accuracy: 0.8884\n",
      "Test size: 24989\n"
     ]
    }
   ],
   "source": [
    "# Testing document classifier\n",
    "\n",
    "print \"Document-doc accuracy: %.4f\" % metrics.accuracy_score(imdb.test.target, doc_clf.predict(imdb.test.bow))\n",
    "print \"Test size: %s\" % imdb.test.bow.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-sentence accuracy: 0.6510\n",
      "Test size: 304952\n"
     ]
    }
   ],
   "source": [
    "# Testing Document-sentence \n",
    "# Sentences take label of the document\n",
    "\n",
    "def doc_to_sents(docs_text, doc_labels, vct):\n",
    "    '''Create bow features for sentences of one document'''\n",
    "    \n",
    "    sent_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    doc_sents = sent_splitter.tokenize_sents(docs_text)\n",
    "    sizes = [len(d) for d in doc_sents]\n",
    "    labels = [[l]*s for l, s in zip(doc_labels, sizes)]\n",
    "    \n",
    "    sents_bow = vct.transform(iterate_sentences(doc_sents))\n",
    "    labels = np.array([l for l in iterate_sentences(labels)])\n",
    "    \n",
    "    return sents_bow, labels\n",
    "\n",
    "test_sx, test_sy = doc_to_sents(imdb.test.data, imdb.test.target, vct_doc)\n",
    "print \"Document-sentence accuracy: %.4f\" % metrics.accuracy_score(test_sy, doc_clf.predict(test_sx))\n",
    "print \"Test size: %s\" % len(test_sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent-doc accuracy: 0.9005\n",
      "Sent-sentence accuracy: 0.6982\n",
      "Test size: 304952\n"
     ]
    }
   ],
   "source": [
    "# Testing sentence to sentence\n",
    "train_sx, train_sy = doc_to_sents(imdb.train.data, imdb.train.target, vct_doc)\n",
    "s2s_clf = LogisticRegression(penalty='l1', C=1)\n",
    "s2s_clf.fit(train_sx, train_sy)\n",
    "print \"Sent-doc accuracy: %.4f\" % metrics.accuracy_score(imdb.test.target, s2s_clf.predict(imdb.test.bow))\n",
    "print \"Sent-sentence accuracy: %.4f\" % metrics.accuracy_score(test_sy, s2s_clf.predict(test_sx))\n",
    "print \"Test size: %s\" % len(test_sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perc. neutrals: 0.243295769942\n",
      "Accuracy: 0.719246558187\n"
     ]
    }
   ],
   "source": [
    "## Neutrality and accuracy of the oracle on sentences, on training data\n",
    "pred_prob = doc_clf.predict_proba(train_sx)\n",
    "pred_sent = doc_clf.predict(train_sx)\n",
    "unc_sent = 1- pred_prob.max(axis=1)\n",
    "thres = 0.42\n",
    "pred_sent[unc_sent > thres] = 2\n",
    "print \"Perc. neutrals: %s\" % (1. * len(pred_sent[unc_sent > thres]) / len(pred_sent))\n",
    "non_neu = pred_sent < 2\n",
    "print \"Accuracy: %s\" % (metrics.accuracy_score(np.array(train_sy)[non_neu], pred_sent[non_neu]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,  TransformerMixin,ClassifierMixin\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack, hstack\n",
    "\n",
    "\n",
    "class ContextVectorizer(BaseEstimator, TransformerMixin):\n",
    "# class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for Vectorizer\"\"\"\n",
    "    def __init__(self, doc_clf, vct, feature_fn,top=10, threshold=.47):\n",
    "        self.doc_clf = doc_clf\n",
    "        self.vct = vct\n",
    "        self.feature_fn = feature_fn \n",
    "        self.top = top\n",
    "        self.threshold = threshold \n",
    "        self.feature_fn = self.feature_context\n",
    "        self.lexicon =  self.get_lexicon(self.doc_clf, top=self.top)\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        x = self.get_training_sentence(documents,  self.feature_fn, \n",
    "                                       threshold=self.threshold)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_lexicon(self, clf, top=10):\n",
    "        '''\n",
    "        Return lexicon of top K terms according to classifier clf. \n",
    "        The function returns feat_index-class pairs\n",
    "        '''\n",
    "    #     feats = np.array(vct.get_feature_names())\n",
    "        coefs = clf.coef_\n",
    "        if coefs.shape[0] == 1:\n",
    "            coefs = [-1 * coefs[0], coefs[0]]\n",
    "\n",
    "        res = []\n",
    "        for ci, cname in enumerate(clf.classes_): # for every class\n",
    "            coef = coefs[ci]\n",
    "            res.extend([(i, cname) for i in np.argsort(coef)[::-1][:top]])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def get_context(self, doc, i):\n",
    "        ''' Get surrounding sentences for context '''\n",
    "        zero_mat = csr_matrix(doc.sent_bow[0].shape, dtype=np.int64)\n",
    "        if i >= len(doc.sent_labels):\n",
    "            raise Exception(\"This doc is not that long.\")\n",
    "        if len(doc.sent_labels) == 1: # there is only one sentence\n",
    "            return np.array([zero_mat, doc.sent_bow[0], zero_mat])\n",
    "        elif i==0: # for the first1\n",
    "            return np.array([zero_mat, doc.sent_bow[0], doc.sent_bow[1]])\n",
    "        elif i == len(doc.sent_labels)-1: # for the last one \n",
    "            return np.array([doc.sent_bow[i-1], doc.sent_bow[i], zero_mat])\n",
    "        else: \n",
    "            return np.array([doc.sent_bow[i-1], doc.sent_bow[i], doc.sent_bow[i+1]])\n",
    "\n",
    "    def get_sentence_label(self,  x, threshold=.4):\n",
    "        '''Get a label or a neutral answer by uncertainty threshold.'''\n",
    "\n",
    "        unc = 1 - self.doc_clf.predict_proba(x).max()\n",
    "        if unc < threshold :\n",
    "            return self.doc_clf.predict(x)\n",
    "        else:\n",
    "            return np.array([2]) ## Neutral label class\n",
    "\n",
    "    def feature_context(self, doc, i,  threshold=.47):\n",
    "        '''Feature function, context and lexicon counts, for one sentence '''\n",
    "        context = self.get_context(doc, i)\n",
    "        sent_lbl = self.get_sentence_label(doc.sent_bow[i], threshold=threshold)\n",
    "\n",
    "        n_lex = len(self.lexicon)\n",
    "        n_feat = (3 * n_lex) + 1  # 2 context sentences and current sentence + label\n",
    "        lex_index = [x[0] for x in self.lexicon]\n",
    "\n",
    "        # Add context sentences + Add sentence label, predicted\n",
    "        lbl_mat = csr_matrix((sent_lbl, ([0] , [0])), shape=(1, 1), dtype=np.int64)\n",
    "        new_feat =  hstack([c[:,lex_index] for c in context]+[lbl_mat])\n",
    "\n",
    "        if new_feat.shape[1] != n_feat:\n",
    "            raise ValueError(\"Oops, convertion did not work. The features are no correct.\")\n",
    "\n",
    "        return new_feat\n",
    "    \n",
    "\n",
    "    def features_per_document(self, doc,  feature_fn,  threshold=.47):\n",
    "        x = vstack([self.feature_fn(doc, i, threshold=threshold) for i in range(len(doc.sent_labels))])\n",
    "        return x\n",
    "\n",
    "    def get_training_sentence(self, documents,  feature_fn,  threshold=.47):\n",
    "        x = vstack([self.features_per_document(d, feature_fn,  threshold=threshold) for d in documents])\n",
    "        return x\n",
    "\n",
    "    def set_top(top):\n",
    "        self.top = top\n",
    "        \n",
    "    def set_unc_threshold(thr):\n",
    "        self.threshold = thr \n",
    "        \n",
    "        \n",
    "class SentenceClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Sentence Classifier. Takes data from ContextVectorizer\"\"\"\n",
    "    def __init__(self):\n",
    "        self.clf = LogisticRegression(penalty='l1', C=1)\n",
    "        \n",
    "        \n",
    "    def __init__(self, doc_clf, vct, feature_fn,top=10, threshold=.47):\n",
    "        self.clf = LogisticRegression(penalty='l1', C=1)\n",
    "        self.converter = ContextVectorizer(doc_clf, vct, feature_fn, top=top, threshold=threshold)\n",
    "        \n",
    "    def convert(self, x):\n",
    "        xx = self.converter.transform(x)\n",
    "        return xx\n",
    "    \n",
    "    def fit(self, x,y):\n",
    "        xx = self.convert(x)\n",
    "        self.clf.fit(xx, y)\n",
    "        self.classes_ = self.clf.classes_\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        xx = self.convert(X)\n",
    "\n",
    "        return self.clf.predict(xx)\n",
    "\n",
    "    def fit2(self, X, y):\n",
    "        self.clf.fit(X,y)\n",
    "        self.classes_ = self.clf.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict2(self, X):\n",
    "        return self.clf.predict(X)  \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        xx = self.convert(X)\n",
    "        return self.clf.predict_proba(xx)\n",
    "\n",
    "    def predict_proba2(self, X):\n",
    "        return self.clf.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24989\n"
     ]
    }
   ],
   "source": [
    "## Convert testing documents \n",
    "te_x, te_y = load_documents(imdb.test, vct_doc, sent_tk)\n",
    "print \"%s\" % (len(te_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "\n",
    "* create a training funtion for data with label from te document \n",
    "* create a function for data from amt with original labels (should work for other data as well)\n",
    "* create a fnction for testing same as training options \n",
    "* create a cv test \n",
    "* create a plot with cv\n",
    "* test base liens\n",
    "* test classifiers\n",
    "* test classifier with simple fieatures\n",
    "* with features for sentiment analysis \n",
    "* with fancy features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Training with Documents\n",
    "def extract_y(x):\n",
    "    yy = []\n",
    "    for d in x:\n",
    "        yy.extend(d.sent_labels)\n",
    "    yy = np.array(yy)\n",
    "    return yy\n",
    "    \n",
    "def train_clf(x, y, clf, convert=False):\n",
    "    if y is None:\n",
    "        yy = extract_y(x)\n",
    "    else: \n",
    "        yy = y\n",
    "        \n",
    "    if convert:\n",
    "        clf.fit(x, yy)\n",
    "    else:\n",
    "        clf.fit2(x,yy)\n",
    "        \n",
    "    return clf\n",
    "\n",
    "def test_clf(x, y, clf, convert=False):\n",
    "    '''\n",
    "    Effective accuracy and neutrality percentage test\n",
    "    :param x: list of Document instances\n",
    "    '''\n",
    "    if y is None:\n",
    "        yy = extract_y(x)\n",
    "    else:\n",
    "        yy = y\n",
    "    \n",
    "    if convert:\n",
    "        pred = clf.predict(x)\n",
    "    else:\n",
    "        pred = clf.predict2(x)\n",
    "\n",
    "    non_neu = pred < 2\n",
    "    return {'accu':metrics.accuracy_score(yy[non_neu], pred[non_neu]), 'neutrals':1- 1.* sum(non_neu) / len(pred), 'train_size':x.shape[0]}\n",
    "\n",
    "s = SentenceClassifier(doc_clf, vct_doc, None, top=2500, threshold=.42)\n",
    "ss = train_clf(x[:10], y[:10], s, convert=True)\n",
    "print test_clf(x[:10],y[:10], ss, convert=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get learning curve data\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "\n",
    "def experiment(data, train_fn, test_fn, clf, train_sizes=np.linspace(.1,1.,5), n_folds=5, seed=12222):\n",
    "#     clf = SentenceClassifier(doc_clf, vct_doc, None, top=2500, threshold=.42)\n",
    "#     cv = cv.ShuffleSplit(len(y), n_iter=5, test_size=.0, random_state=12345)\n",
    "    cross_val = cv.KFold(len(data), n_folds=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    test_scores = []\n",
    "        \n",
    "    for train_index, test_index in cross_val:\n",
    "        trial_score = []\n",
    "        print \"Train size: %s, Test size: %s\" % (len(train_index), len(test_index))\n",
    "        \n",
    "        # convert test once\n",
    "        test_x = clf.convert(x[test_index])\n",
    "        test_y = extract_y(x[test_index])\n",
    "\n",
    "        # convert train once \n",
    "        sent_size = [d.sent_bow.shape[0] for d in x[train_index[:max(train_sizes)]]]\n",
    "        train_x = clf.convert(x[train_index[:max(train_sizes)]])\n",
    "\n",
    "        for size in train_sizes:\n",
    "            print \"Size: %s\" % size\n",
    "            adj_size= sum(sent_size[:size])\n",
    "\n",
    "#             trained = train_fn(x[:size], clf, convert=True)\n",
    "\n",
    "            trained = train_fn(train_x[:adj_size], train_y[:adj_size],clf, convert=False)\n",
    "            trial_score.append(test_fn(test_x, test_y, trained,convert=False))\n",
    "            print \"Accuracy: %s\" % trial_score[-1]\n",
    "        test_scores.append(trial_score)\n",
    "    \n",
    "    test_scores_mean = np.mean([t['accu'] for t in test_scores], axis=0)\n",
    "    test_scores_std = np.std([t['accu'] for t in test_scores], axis=0)\n",
    "    \n",
    "\n",
    "    return test_scores, test_scores_mean, test_scores_std \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(311296, 15001)\n",
      "(311296L,)\n",
      "24991\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "train_x = s.converter.transform(x)\n",
    "train_y = extract_y(x)\n",
    "sent_len = np.array([d.sent_bow.shape[0] for d in x])\n",
    "print train_x.shape\n",
    "print train_y.shape\n",
    "print len(sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train_x.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 Accuracy: {'accu': 0.54768388031656556, 'train_size': 62799, 'neutrals': 0.0}\n",
      "585 Accuracy: {'accu': 0.57684039554769984, 'train_size': 62799, 'neutrals': 0.0}\n",
      "1242 Accuracy: {'accu': 0.5898660806700744, 'train_size': 62799, 'neutrals': 0.0}\n",
      "4270 Accuracy: {'accu': 0.64362489848564464, 'train_size': 62799, 'neutrals': 0.0}\n",
      "7455 Accuracy: {'accu': 0.66966034490995074, 'train_size': 62799, 'neutrals': 0.0}\n",
      "10696 Accuracy: {'accu': 0.68642812783643048, 'train_size': 62799, 'neutrals': 0.0}\n",
      "13889 Accuracy: {'accu': 0.70104619500310517, 'train_size': 62799, 'neutrals': 0.0}\n",
      "16818 Accuracy: {'accu': 0.70725648497587545, 'train_size': 62799, 'neutrals': 0.0}\n",
      "20230 Accuracy: {'accu': 0.71362601315307572, 'train_size': 62799, 'neutrals': 0.0}\n",
      "23446 Accuracy: {'accu': 0.71888087389926592, 'train_size': 62799, 'neutrals': 0.0}\n",
      "26497 Accuracy: {'accu': 0.72391280115925416, 'train_size': 62799, 'neutrals': 0.0}\n",
      "29489 Accuracy: {'accu': 0.72687463176165223, 'train_size': 62799, 'neutrals': 0.0}\n",
      "32352 Accuracy: {'accu': 0.73260720712113248, 'train_size': 62799, 'neutrals': 0.0}\n",
      "35495 Accuracy: {'accu': 0.73410404624277459, 'train_size': 62799, 'neutrals': 0.0}\n",
      "100 Accuracy: {'accu': 0.52157008861313159, 'train_size': 61729, 'neutrals': 0.0}\n",
      "577 Accuracy: {'accu': 0.56466166631567005, 'train_size': 61729, 'neutrals': 0.0}\n",
      "1250 Accuracy: {'accu': 0.61235399893081044, 'train_size': 61729, 'neutrals': 0.0}\n",
      "4201 Accuracy: {'accu': 0.65627176853666835, 'train_size': 61729, 'neutrals': 0.0}\n",
      "7236 Accuracy: {'accu': 0.67021983184564793, 'train_size': 61729, 'neutrals': 0.0}\n",
      "10651 Accuracy: {'accu': 0.68674366991203484, 'train_size': 61729, 'neutrals': 0.0}\n",
      "13884 Accuracy: {'accu': 0.69829415671726414, 'train_size': 61729, 'neutrals': 0.0}\n",
      "16973 Accuracy: {'accu': 0.70653987590921608, 'train_size': 61729, 'neutrals': 0.0}\n",
      "20506 Accuracy: {'accu': 0.71324661018322022, 'train_size': 61729, 'neutrals': 0.0}\n",
      "23727 Accuracy: {'accu': 0.71810656255568694, 'train_size': 61729, 'neutrals': 0.0}\n",
      "26795 Accuracy: {'accu': 0.72183252604124482, 'train_size': 61729, 'neutrals': 0.0}\n",
      "29724 Accuracy: {'accu': 0.72413290349754578, 'train_size': 61729, 'neutrals': 0.0}\n",
      "32652 Accuracy: {'accu': 0.72944645142477604, 'train_size': 61729, 'neutrals': 0.0}\n",
      "35766 Accuracy: {'accu': 0.73348021189392343, 'train_size': 61729, 'neutrals': 0.0}\n",
      "116 Accuracy: {'accu': 0.53197016012797571, 'train_size': 63137, 'neutrals': 0.0}\n",
      "533 Accuracy: {'accu': 0.57810792403820266, 'train_size': 63137, 'neutrals': 0.0}\n",
      "1202 Accuracy: {'accu': 0.58783280802065352, 'train_size': 63137, 'neutrals': 0.0}\n",
      "4066 Accuracy: {'accu': 0.64958740516654256, 'train_size': 63137, 'neutrals': 0.0}\n",
      "7439 Accuracy: {'accu': 0.66886294882556985, 'train_size': 63137, 'neutrals': 0.0}\n",
      "10582 Accuracy: {'accu': 0.68023504442719795, 'train_size': 63137, 'neutrals': 0.0}\n",
      "13705 Accuracy: {'accu': 0.69089440423206683, 'train_size': 63137, 'neutrals': 0.0}\n",
      "16784 Accuracy: {'accu': 0.69897207659534033, 'train_size': 63137, 'neutrals': 0.0}\n",
      "20303 Accuracy: {'accu': 0.7060835959896733, 'train_size': 63137, 'neutrals': 0.0}\n",
      "23426 Accuracy: {'accu': 0.70993236929217418, 'train_size': 63137, 'neutrals': 0.0}\n",
      "26431 Accuracy: {'accu': 0.71563425566624961, 'train_size': 63137, 'neutrals': 0.0}\n",
      "29459 Accuracy: {'accu': 0.7181525888147996, 'train_size': 63137, 'neutrals': 0.0}\n",
      "32437 Accuracy: {'accu': 0.72523243106260993, 'train_size': 63137, 'neutrals': 0.0}\n",
      "35635 Accuracy: {'accu': 0.72699051269461645, 'train_size': 63137, 'neutrals': 0.0}\n",
      "103 Accuracy: {'accu': 0.55511607632946947, 'train_size': 61942, 'neutrals': 0.0}\n",
      "567 Accuracy: {'accu': 0.5821090697749508, 'train_size': 61942, 'neutrals': 0.0}\n",
      "1237 Accuracy: {'accu': 0.59198928029446907, 'train_size': 61942, 'neutrals': 0.0}\n",
      "4171 Accuracy: {'accu': 0.64899422039972876, 'train_size': 61942, 'neutrals': 0.0}\n",
      "7540 Accuracy: {'accu': 0.67270995447353976, 'train_size': 61942, 'neutrals': 0.0}\n",
      "10677 Accuracy: {'accu': 0.69188918665848698, 'train_size': 61942, 'neutrals': 0.0}\n",
      "13805 Accuracy: {'accu': 0.70036485744728938, 'train_size': 61942, 'neutrals': 0.0}\n",
      "16821 Accuracy: {'accu': 0.7090181137192858, 'train_size': 61942, 'neutrals': 0.0}\n",
      "20411 Accuracy: {'accu': 0.71420038100158212, 'train_size': 61942, 'neutrals': 0.0}\n",
      "23668 Accuracy: {'accu': 0.71602466823802913, 'train_size': 61942, 'neutrals': 0.0}\n",
      "26691 Accuracy: {'accu': 0.72291821381292176, 'train_size': 61942, 'neutrals': 0.0}\n",
      "29771 Accuracy: {'accu': 0.7270349681960544, 'train_size': 61942, 'neutrals': 0.0}\n",
      "32768 Accuracy: {'accu': 0.73081269574763486, 'train_size': 61942, 'neutrals': 0.0}\n",
      "35859 Accuracy: {'accu': 0.7342675406024991, 'train_size': 61942, 'neutrals': 0.0}\n",
      "108 Accuracy: {'accu': 0.52654444066203054, 'train_size': 61689, 'neutrals': 0.0}\n",
      "587 Accuracy: {'accu': 0.57478642869879559, 'train_size': 61689, 'neutrals': 0.0}\n",
      "1212 Accuracy: {'accu': 0.58178929792993894, 'train_size': 61689, 'neutrals': 0.0}\n",
      "4236 Accuracy: {'accu': 0.64773298318987182, 'train_size': 61689, 'neutrals': 0.0}\n",
      "7573 Accuracy: {'accu': 0.67832190503979639, 'train_size': 61689, 'neutrals': 0.0}\n",
      "10888 Accuracy: {'accu': 0.6930571090469938, 'train_size': 61689, 'neutrals': 0.0}\n",
      "14183 Accuracy: {'accu': 0.69942777480588114, 'train_size': 61689, 'neutrals': 0.0}\n",
      "17202 Accuracy: {'accu': 0.70785715443596098, 'train_size': 61689, 'neutrals': 0.0}\n",
      "20569 Accuracy: {'accu': 0.71380635121334435, 'train_size': 61689, 'neutrals': 0.0}\n",
      "23600 Accuracy: {'accu': 0.71962586522718797, 'train_size': 61689, 'neutrals': 0.0}\n",
      "26443 Accuracy: {'accu': 0.7242944447146169, 'train_size': 61689, 'neutrals': 0.0}\n",
      "29528 Accuracy: {'accu': 0.72818492762080755, 'train_size': 61689, 'neutrals': 0.0}\n",
      "32517 Accuracy: {'accu': 0.73304803125354601, 'train_size': 61689, 'neutrals': 0.0}\n",
      "35772 Accuracy: {'accu': 0.73719787968681616, 'train_size': 61689, 'neutrals': 0.0}\n"
     ]
    }
   ],
   "source": [
    "def sent_iterator(data, sizes, order):\n",
    "    for i in order:\n",
    "        yield get_sents(data, sizes,i)\n",
    "        \n",
    "def experiment_v2(x, y, train_fn, test_fn, clf, train_sizes=np.linspace(.1,1.,5), n_folds=5, seed=12222, sent_size=None):\n",
    "\n",
    "    cross_val = cv.KFold(len(sent_size), n_folds=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    test_scores = []\n",
    "\n",
    "    for train_index, test_index in cross_val:\n",
    "        trial_score = []\n",
    "        #stack matrix in order of training\n",
    "        trainx = vstack([d for d in sent_iterator(x, sent_size, train_index[:max(train_sizes)])])\n",
    "        trainy = np.hstack((d for d in sent_iterator(y, sent_size, train_index[:max(train_sizes)])))\n",
    "\n",
    "        testx = vstack([d for d in sent_iterator(x, sent_size, test_index)])\n",
    "        testy = np.hstack((d for d in sent_iterator(y, sent_size, test_index)))\n",
    "        \n",
    "        for size in train_sizes:\n",
    "            adj_size = sum([s for s in sent_size[train_index[:size]]])\n",
    "            print adj_size, \n",
    "            trained = train_fn(trainx[:adj_size], trainy[:adj_size],clf, convert=False)\n",
    "            \n",
    "            trial_score.append(test_fn(testx, testy, trained, convert=False))\n",
    "            \n",
    "            print \"Accuracy: %s\" % trial_score[-1]\n",
    "        test_scores.append(trial_score)\n",
    "#     print test_scores\n",
    "    test_scores_mean = np.mean([[s['accu'] for s in t] for t in test_scores], axis=0)\n",
    "    test_scores_std = np.std([[s['accu'] for s in t] for t in test_scores], axis=0)\n",
    "    \n",
    "\n",
    "    return test_scores, test_scores_mean, test_scores_std\n",
    "sizes = [10, 50] + range(100, 3000, 250)\n",
    "scores, mean, std = experiment_v2(train_x,train_y, train_clf, test_clf, s,sent_size=sent_len,n_folds=5, train_sizes=sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 16 13 15]\n",
      "[11 27 40 55]\n",
      "(15, 15001)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sents(data, sizes, i):\n",
    "    ranges =  np.cumsum(sizes)\n",
    "#     print 0 if i==0 else ranges[i-1],ranges[i]\n",
    "    return data[0 if i==0 else ranges[i-1]:ranges[i]]\n",
    "\n",
    "# print [get_sents(r,[10,5,5,10,20], i) for i in range(0,5,1)]\n",
    "print sent_len[:4]\n",
    "print np.cumsum(sent_len[:4])\n",
    "print get_sents(train_x, sent_len, 3).shape\n",
    "type(x[0].sent_bow)\n",
    "# print np.cumsum(sent_len[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAESCAYAAADe2fNYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nssRijDEmoiyxGGOMiShLLMYYYyLq/wHz3kA15/4iJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5d17ba90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def exp_plot(results, sizes=None):\n",
    "    \n",
    "    for m, res in results.items():\n",
    "        avg = res['mean']\n",
    "        std = res['std']# / np.sqrt(5)\n",
    "#         plt.plot(sizes,avg,  label=m)\n",
    "        plt.errorbar(sizes,avg,yerr=std, label=m)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training Size - |D|')\n",
    "    plt.ylabel('Accuracy of Student')\n",
    "            \n",
    "exp_plot({'method1':{'mean':mean, 'std':std}}, sizes=sizes)\n",
    "exp_plot({'method1':{'mean':np.mean([[s['accu'] for s in t] for t in test_scores], axis=0), 'std':std}}, sizes=sizes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceClassifier(doc_clf=None, feature_fn=None, threshold=None, top=None,\n",
      "          vct=None)\n"
     ]
    }
   ],
   "source": [
    "print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Feature Spaces \n",
    "\n",
    "## 1. Sentiment-based Features\n",
    "\n",
    "Based on (McDonald, 2011): \n",
    "\n",
    "Features are s :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train sentence classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "context_vect = ContextVectorizer(doc_clf, vct_doc, feature_context ,top=2500, threshold=.45)\n",
    "sent_cla = SentenceClassifier()\n",
    "\n",
    "## Get the training data\n",
    "# Get all and train classifier, fully trained\n",
    "ss2s_clf = Pipeline(steps=[('context', context_vect),('estimator', sent_cla)])\n",
    "ss2s_clf.fit(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Get the test data\n",
    "# Transform all test\n",
    "text_x,test_y = load_documents(imdb.test, vct_doc, sent_tk)\n",
    "testx_ss2s = context_vect.transform(test_x)\n",
    "\n",
    "# print \"Sent-doc accuracy: %.4f\" % metrics.accuracy_score(testx_ss2s[:,-1], s2s_clf.predict(testx_ss2s[:,:-1]))\n",
    "print \"Sent-sentence accuracy: %.4f\" % metrics.accuracy_score(testx_ss2s[:,-1], s2s_clf.predict(testx_ss2s[:,:-1]))\n",
    "print \"Test size: %s\" % (testx_ss2s.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n",
    "\n",
    "sent_x\n",
    "\n",
    "# # prueba =  context_vect.transform(x[:10])\n",
    "# # print prueba[:,-1]\n",
    "\n",
    "\n",
    "# # prueba2 = get_training_sentence(x[:10], doc_clf, feature_context, top=10, threshold=.47)\n",
    "# # print prueba2[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sent_cla = SentenceClassifier()\n",
    "clf_sent = Pipeline(steps=[('context', context_vect),('estimator', sent_cla)])\n",
    "\n",
    "# clf_sent.fit(sub_x, sub_y)\n",
    "\n",
    "# t_x = context_vect.transform(sub_x)\n",
    "print metrics.accuracy_score(t_x[:,-1], sent_cla.predict(t_x))\n",
    "print metrics.accuracy_score(t_x[:,-1], clf_sent.predict(sub_x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## train on bootstraps, test on amt data sentences\n",
    "#_# train on bootstraps, test on sentences as documents \n",
    "t_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on AMT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "from utilities.amt_datautils import load_amt_imdb\n",
    "amt = load_amt_imdb(IMDB_DATA, shuffle=True, rnd=1928374, amt_labels='labels')  # should bring with training labels as the amt annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_documents_amt(data, vct, sent_tk ):\n",
    "    # sents, sents_bow, sents_lbl, doc_lbl\n",
    "    \n",
    "    sents_doc = [d.split('THIS_IS_A_SEPARATOR') for d in data.data]\n",
    "    sents_bow = [vct.transform(d) for d in sents_doc]\n",
    "    sents_lbl = data.target\n",
    "    \n",
    "    x = np.array([Document(a,b,c,d) for a,b,c,d in zip(sents_doc, sents_bow, sents_lbl, data.doctarget)])\n",
    "    y = data.doctarget\n",
    "    return x,y\n",
    "\n",
    "\n",
    "print amt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print amt.train.keys()\n",
    "print len(amt.train.doctarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert document to new feature space\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import DictVectorizer\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     row = [0] * n_features\n",
    "#     col = [l[0] for l in lexicon]\n",
    "#     counts = d.sent_bow[:,col]\n",
    "    \n",
    "#     data= counts.sum(axis=0)\n",
    "#     return csr_matrix( (data,(row,col)), shape=(1,n_features) )\n",
    "\n",
    "#     csr_matrix()\n",
    "\n",
    "def feature_simple_counts(doc, i, doc_clf):\n",
    "    '''Feature function bag of words, no context'''\n",
    "    return doc.sent_bow[i]\n",
    "    \n",
    "def featurize(documents, labels, clf_d, feature_fn):\n",
    "    '''Create a feature vector from documents'''\n",
    "\n",
    "    lexicon = get_lexicon(clf_d, top=10) \n",
    "    x = vstack((feature_fn(d, lexicon, clf_d) for d in documents))\n",
    "    return x\n",
    "    \n",
    "def feature_counts(doc, lexicon, clf_d):\n",
    "    n_features = len(lexicon) * 4 + 1\n",
    "    row = [0] * n_features\n",
    "    col = [l[0] for l in lexicon]\n",
    "    counts = d.sent_bow[:,col]\n",
    "    \n",
    "    data= counts.sum(axis=0)\n",
    "    return csr_matrix( (data,(row,col)), shape=(1,n_features) )\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "1. Load train and test\n",
    "1. Features for train:\n",
    "    1. for documents: vectorizer like before\n",
    "    1. for sentences: lexicon counts\n",
    "1. features for test: same as for sentences (this is amt data)\n",
    "1. for every size of the bootstrap\n",
    "    1. train a document classifier\n",
    "    1. obtain sentences and featurize\n",
    "    1. test document\n",
    "    1. test sentence\n",
    "    1. save results\n",
    "    \n",
    "### Features per sentence\n",
    "\n",
    "1. For every document\n",
    "    1. for every sentence\n",
    "        1. get context, document, and label, and lexicon\n",
    "        1. build a vector\n",
    "        1. return vecotr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def experiment(data, vct, runs, rnd=123):\n",
    "    x_doc, y_doc = load_documents(data, vct)\n",
    "    for train, test in cv:\n",
    "        clf_d = LogisticRegression(penalty=\"l1\", C=1)\n",
    "        clf_s = LogisticRegression(penalty=\"l1\", C=1)\n",
    "        \n",
    "        clf_d.fit(data.train.bow[train], data.train.target[train])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_predict():\n",
    "    pass\n",
    "def lr_fit():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
