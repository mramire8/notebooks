{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with data from <https://github.com/oscartackstrom/sentence-sentiment-data>.\n",
    "\n",
    "One document looks like this:\n",
    "\n",
    "```\n",
    "books_neg_2\n",
    "books   2\n",
    "neg     The book is disproportionally focused on single and multilayer feedforward networks.\n",
    "neg     And though the book puts great emphasis on mathematics and even includes a big section on important mathematical background knowledge, it contains to many errors in the mathematical formulas, so they are of little use.\n",
    "neg     The author hasn't even taken the trouble to put up an errata list.\n",
    "neg     Finally, for the beginner there are not enough conceptual clues on what is actually going on and it is hard to form any mental model of the underlying processes.\n",
    "neg     There are better books.\n",
    "neu     For an introduction read Neural Networks by Kevin Gurney. He puts great emphasis on conceptual understanding.\n",
    "neu     For further studies there is Neural Networks by Simon Haykin, which has the mathematics.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import array as npa\n",
    "import re\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 294 documents with 3836 sentences\n",
      "document label distribution: Counter({'neg': 99, 'neu': 98, 'pos': 97})\n",
      "sentence label distribution: Counter({'neg': 1320, 'nr': 1147, 'pos': 923, 'neu': 272, 'mix': 174})\n"
     ]
    }
   ],
   "source": [
    "class Document(object):\n",
    "    def __init__(self, raw_text):\n",
    "        self.sentences = raw_text.split(\"\\n\\n\")\n",
    "        self.doc_label = self.sentences[0]\n",
    "        self.sentences = npa(self.sentences[2:]) # ignoring product type\n",
    "        self.doc_label = re.findall(r'^.*_(.+?)_.*', self.doc_label)[0]\n",
    "        self.sent_labels = npa([s.split('\\t')[0] for s in self.sentences])\n",
    "        self.sentences = npa([s.split('\\t')[1] for s in self.sentences])\n",
    "        \n",
    "\n",
    "def read_data(filename):\n",
    "    all_data = '\\n'.join(open(filename).readlines())\n",
    "    documents = [Document(d) for d in all_data.split('\\n\\n\\n\\n')]\n",
    "    return npa(documents)\n",
    "\n",
    "def count_sentences(docs):\n",
    "    return sum(len(d.sent_labels) for d in docs)\n",
    "\n",
    "def count_sentence_labels(docs):\n",
    "    counts = Counter()\n",
    "    for d in docs:\n",
    "        counts.update(d.sent_labels)\n",
    "    return counts\n",
    "\n",
    "documents = read_data('finegrained.txt')\n",
    "print('read %d documents with %d sentences' % (len(documents), count_sentences(documents)))\n",
    "print('document label distribution: %s' % Counter(d.doc_label for d in documents))\n",
    "print('sentence label distribution: %s' % count_sentence_labels(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after converting [mix|nr]->neu, sentence label distribution: Counter({'neu': 1593, 'neg': 1320, 'pos': 923})\n"
     ]
    }
   ],
   "source": [
    "# set mix and nr -> neu (as in Tackstrom & McDonald)\n",
    "def set_nr_to_neu(documents):\n",
    "    for d in documents:\n",
    "        d.sent_labels = npa(['neu' if s in ['mix', 'nr'] else s for s in d.sent_labels])\n",
    "        \n",
    "set_nr_to_neu(documents)\n",
    "print('after converting [mix|nr]->neu, sentence label distribution: %s' % count_sentence_labels(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 documents remain\n",
      "document label distribution: Counter({'neg': 99, 'pos': 97})\n",
      "sentence label distribution: Counter({'neu': 896, 'neg': 837, 'pos': 727})\n"
     ]
    }
   ],
   "source": [
    "# remove neutral documents.\n",
    "documents = npa([d for d in documents if d.doc_label != 'neu'])\n",
    "print('%d documents remain' % len(documents))\n",
    "print('document label distribution: %s' % Counter(d.doc_label for d in documents))\n",
    "print('sentence label distribution: %s' % count_sentence_labels(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3756 features\n"
     ]
    }
   ],
   "source": [
    "def featurize(documents):\n",
    "    vec = CountVectorizer(min_df=2, token_pattern=re.compile(r'(?u)\\b\\w+\\b'))\n",
    "    def iterate_sentences(documents):\n",
    "        for d in documents:\n",
    "            for s in d.sentences:\n",
    "                yield s\n",
    "                \n",
    "    X = vec.fit_transform(iterate_sentences(documents))\n",
    "    start = 0\n",
    "    for d in documents:\n",
    "        end = start + len(d.sentences)\n",
    "        d.sentence_matrix = X[start:end]\n",
    "        start = end\n",
    "    return vec\n",
    "\n",
    "vec = featurize(documents)\n",
    "print('%d features' % len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_feats(clf, vec, n=10):\n",
    "    feats = npa(vec.get_feature_names())\n",
    "    coefs = clf.coef_\n",
    "    if coefs.shape[0] == 1:\n",
    "        coefs = [-1 * coefs[0], coefs[0]]\n",
    "    for ci, cname in enumerate(clf.classes_):\n",
    "        coef = coefs[ci]\n",
    "        print('\\n' + cname + '\\n-----')\n",
    "        print('\\n'.join('%s=%.4f' % (feats[i], coef[i]) for i in np.argsort(coef)[::-1][:n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do cross-validation, reporting how often the sentence returned\n",
    "by the student has a true label that matches the document label.\n",
    "\"\"\"\n",
    "def expt(documents, fit_fn, predict_fn, n_folds=5, seed=1234):\n",
    "    cv = KFold(len(documents), n_folds=n_folds, shuffle=True, random_state=seed)\n",
    "    doc_preds = []\n",
    "    sent_preds = []\n",
    "    doc_truths = []\n",
    "    sent_truths = []\n",
    "    for traini, testi in cv:\n",
    "        train_docs = documents[traini]\n",
    "        test_docs = documents[testi]\n",
    "        clf = fit_fn(train_docs, seed)\n",
    "        this_doc_preds, this_sent_preds = predict_fn(clf, test_docs, seed)\n",
    "        doc_preds.extend(this_doc_preds)\n",
    "        sent_preds.extend(this_sent_preds)\n",
    "        doc_truths.extend(d.doc_label for d in test_docs)\n",
    "        sent_truths.extend(np.concatenate([d.sent_labels for d in test_docs]))\n",
    "    doc_acc = accuracy_score(doc_preds, doc_truths)\n",
    "    doc_preds = npa(doc_preds)\n",
    "    doc_truths = npa(doc_truths)\n",
    "    nonneutr = [i for i, l in enumerate(doc_preds) if l != 'neu']\n",
    "    print('oracle acc=%.3f acc w/o neutral=%.3f pct neutral=%.3f' %\n",
    "          (doc_acc,\n",
    "           accuracy_score(doc_preds[nonneutr], doc_truths[nonneutr]),\n",
    "           1 - len(nonneutr)/len(doc_preds)))\n",
    "    #print('document accuracy:')\n",
    "    #print(classification_report(doc_truths, doc_preds))\n",
    "    print(confusion(doc_truths, doc_preds, sorted(set(doc_preds))))\n",
    "    sent_acc = accuracy_score(sent_preds, sent_truths)\n",
    "    print('sentence accuracy=%.3f' % sent_acc)\n",
    "    #print(classification_report(sent_truths, sent_preds))\n",
    "    # retrain on all.\n",
    "    clf = fit_fn(documents, seed)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def confusion(truths, preds, labels):\n",
    "    m = confusion_matrix(truths, preds, labels=labels)\n",
    "    labels = [l[:4] for l in labels]\n",
    "    m = np.vstack((labels, m))\n",
    "    m = np.hstack((np.matrix([''] + list(labels)).T, m))\n",
    "    return tabulate(m.tolist(), headers='firstrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracle acc=0.643 acc w/o neutral=0.962 pct neutral=0.332\n",
      "       neg    neu    pos\n",
      "---  -----  -----  -----\n",
      "neg     65     31      3\n",
      "neu      0      0      0\n",
      "pos      2     34     61\n",
      "sentence accuracy=0.400\n",
      "\n",
      "neg\n",
      "-----\n",
      "were=1.6503\n",
      "movie=1.6080\n",
      "waste=1.4356\n",
      "bag=1.3727\n",
      "novel=1.3407\n",
      "weapons=1.2985\n",
      "please=1.2609\n",
      "beyonce=1.1895\n",
      "headset=1.1507\n",
      "poor=1.1432\n",
      "\n",
      "pos\n",
      "-----\n",
      "battery=1.3204\n",
      "perfect=1.3037\n",
      "sing=1.2362\n",
      "camera=1.2040\n",
      "spiritual=1.1938\n",
      "ceedee=1.1789\n",
      "isn=1.1527\n",
      "its=1.1213\n",
      "sense=1.0525\n",
      "zoom=1.0349\n"
     ]
    }
   ],
   "source": [
    "# Use document label as sentence label at training time (DaS=\"document as sentence\")\n",
    "def get_label_of_most_confident_sentence(probas, doc):\n",
    "    # Find true sentence label of most confident sentence.\n",
    "    # This simulates what the oracle would return if we\n",
    "    # returned the most confident sentence.\n",
    "    return doc.sent_labels[probas.max(axis=1).argmax()]\n",
    "\n",
    "def fit_lr(documents, seed):\n",
    "    X = vstack((d.sentence_matrix for d in documents))\n",
    "    # duplicate doc label for each sentence.\n",
    "    y = np.concatenate([[d.doc_label] * len(d.sentences) for d in documents])\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def predict_lr(clf, documents, seed, thresh=.6):\n",
    "    # label as neutral sentences with max posterior < thresh.\n",
    "    doc_preds = []\n",
    "    sent_preds = []\n",
    "    for d in documents:\n",
    "        probas = clf.predict_proba(d.sentence_matrix)\n",
    "        preds = clf.predict(d.sentence_matrix)\n",
    "        doc_preds.append(get_label_of_most_confident_sentence(probas, d))\n",
    "        neu_idx = [i for i, p in enumerate(probas) if p.max() < thresh]\n",
    "        preds[neu_idx] = 'neu'\n",
    "        sent_preds.extend(preds)\n",
    "    return doc_preds, sent_preds\n",
    "\n",
    "clf = expt(documents, fit_lr, predict_lr)\n",
    "print_top_feats(clf, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracle acc=0.633 acc w/o neutral=0.905 pct neutral=0.301\n",
      "       neg    neu    pos\n",
      "---  -----  -----  -----\n",
      "neg     62     30      7\n",
      "neu      0      0      0\n",
      "pos      6     29     62\n",
      "sentence accuracy=0.485\n",
      "\n",
      "neg\n",
      "-----\n",
      "boring=1.7935\n",
      "hear=1.3839\n",
      "terrible=1.3595\n",
      "waste=1.2567\n",
      "unfortunately=1.2460\n",
      "annoying=1.2202\n",
      "money=1.2157\n",
      "belly=1.2086\n",
      "put=1.1926\n",
      "felt=1.1855\n",
      "\n",
      "neu\n",
      "-----\n",
      "watch=1.6561\n",
      "isn=1.4517\n",
      "released=1.4494\n",
      "x=1.3380\n",
      "box=1.2395\n",
      "wise=1.2314\n",
      "wants=1.2184\n",
      "dance=1.1918\n",
      "mean=1.1635\n",
      "women=1.1120\n",
      "\n",
      "pos\n",
      "-----\n",
      "enjoyed=1.6444\n",
      "perfect=1.6117\n",
      "funny=1.4661\n",
      "negative=1.2835\n",
      "amazing=1.2538\n",
      "excellent=1.2383\n",
      "great=1.1795\n",
      "easy=1.1595\n",
      "large=1.1393\n",
      "match=1.1340\n"
     ]
    }
   ],
   "source": [
    "# Use true sentence label as sentence label (Fine)\n",
    "def fit_lr_sent_labels(documents, seed):\n",
    "    X = vstack((d.sentence_matrix for d in documents))\n",
    "    y = np.concatenate([d.sent_labels for d in documents])\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "def predict_lr_sent_labels(clf, documents, seed):\n",
    "    doc_preds = []\n",
    "    sent_preds = []\n",
    "    # ignore neutral labels.\n",
    "    neu_idx = np.where(clf.classes_ == 'neu')[0]\n",
    "    for d in documents:\n",
    "        probas = clf.predict_proba(d.sentence_matrix)\n",
    "        probas[:,neu_idx] = 0\n",
    "        preds = clf.predict(d.sentence_matrix)\n",
    "        doc_preds.append(get_label_of_most_confident_sentence(probas, d))\n",
    "        sent_preds.extend(preds)\n",
    "    return doc_preds, sent_preds\n",
    "\n",
    "clf = expt(documents, fit_lr_sent_labels, predict_lr_sent_labels)\n",
    "print_top_feats(clf, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting with sentence labels actually hurts accuracy slightly, while helping neutrality.\n",
    "\n",
    "I think part of what's happening is that Fine has a better model of neutrality, since it actually has labeled data for it. This sacrifices somewhat pos/neg accuracy.\n",
    "\n",
    "Looking at the top terms, Fine looks a lot better than DaS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracle acc=0.648 acc w/o neutral=0.969 pct neutral=0.332\n",
      "       neg    neu    pos\n",
      "---  -----  -----  -----\n",
      "neg     67     31      1\n",
      "neu      0      0      0\n",
      "pos      3     34     60\n",
      "sentence accuracy=0.400\n"
     ]
    }
   ],
   "source": [
    "# Cheat by returning the sentence with the maximum predicted probability\n",
    "# according to the *known* document label.\n",
    "def predict_lr_cheat_doc(clf, documents, seed, thresh=.6):\n",
    "    # label as neutral sentences with max posterior < thresh.\n",
    "    doc_preds = []\n",
    "    sent_preds = []\n",
    "    for d in documents:\n",
    "        probas = clf.predict_proba(d.sentence_matrix)\n",
    "        preds = clf.predict(d.sentence_matrix)\n",
    "        true_idx = np.where(clf.classes_==d.doc_label)[0]\n",
    "        doc_preds.append(d.sent_labels[probas[:, true_idx].argmax()])\n",
    "        neu_idx = [i for i, p in enumerate(probas) if p.max() < thresh]\n",
    "        preds[neu_idx] = 'neu'\n",
    "        sent_preds.extend(preds)\n",
    "    return doc_preds, sent_preds\n",
    "\n",
    "clf = expt(documents, fit_lr, predict_lr_cheat_doc)\n",
    "# print_top_feats(clf, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only helps DaS by correcting exactly one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracle acc=0.735 acc w/o neutral=0.954 pct neutral=0.230\n",
      "       neg    neu    pos\n",
      "---  -----  -----  -----\n",
      "neg     72     25      2\n",
      "neu      0      0      0\n",
      "pos      5     20     72\n",
      "sentence accuracy=0.485\n"
     ]
    }
   ],
   "source": [
    "# Cheating expt using sentence labels at training time.\n",
    "def predict_lr_sent_labels_cheat_doc(clf, documents, seed):\n",
    "    # label as neutral sentences with max posterior < thresh.\n",
    "    doc_preds = []\n",
    "    sent_preds = []\n",
    "    for d in documents:\n",
    "        probas = clf.predict_proba(d.sentence_matrix)\n",
    "        preds = clf.predict(d.sentence_matrix)\n",
    "        true_idx = np.where(clf.classes_==d.doc_label)[0]\n",
    "        doc_preds.append(d.sent_labels[probas[:, true_idx].argmax()])\n",
    "        sent_preds.extend(preds)\n",
    "    return doc_preds, sent_preds\n",
    "\n",
    "clf = expt(documents, fit_lr_sent_labels, predict_lr_sent_labels_cheat_doc)\n",
    "# print_top_feats(clf, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheating helps Fine a lot more than DaS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracle acc=0.628 acc w/o neutral=0.969 pct neutral=0.352\n",
      "       neg    neu    pos\n",
      "---  -----  -----  -----\n",
      "neg     64     33      2\n",
      "neu      0      0      0\n",
      "pos      2     36     59\n",
      "sentence accuracy=0.400\n"
     ]
    }
   ],
   "source": [
    "# Returning the sentence with the maximum predicted probability\n",
    "# according to the *predicted* document label.\n",
    "# The predicted document label is obtained by summing the feature vectors\n",
    "# for all sentences and classifying the resulting vector.\n",
    "def predict_lr_pred_doc(clf, documents, seed, thresh=.6):\n",
    "    doc_preds = []\n",
    "    sent_preds = []\n",
    "    for d in documents:\n",
    "        # sum sentence feature vectors and predict.\n",
    "        this_doc_pred = clf.predict(d.sentence_matrix.sum(axis=0))\n",
    "        probas = clf.predict_proba(d.sentence_matrix)\n",
    "        preds = clf.predict(d.sentence_matrix)\n",
    "        true_idx = np.where(clf.classes_==this_doc_pred)[0]\n",
    "        doc_preds.append(d.sent_labels[probas[:, true_idx].argmax()])\n",
    "        neu_idx = [i for i, p in enumerate(probas) if p.max() < thresh]\n",
    "        preds[neu_idx] = 'neu'\n",
    "        sent_preds.extend(preds)\n",
    "    return doc_preds, sent_preds\n",
    "\n",
    "clf = expt(documents, fit_lr, predict_lr_pred_doc)\n",
    "# print_top_feats(clf, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to ignoring document label, minimal improvement to accuracy, hurts neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracle acc=0.663 acc w/o neutral=0.909 pct neutral=0.270\n",
      "       neg    neu    pos\n",
      "---  -----  -----  -----\n",
      "neg     65     26      8\n",
      "neu      0      0      0\n",
      "pos      5     27     65\n",
      "sentence accuracy=0.485\n"
     ]
    }
   ],
   "source": [
    "# Returning the sentence with the maximum predicted probability\n",
    "# according to the *predicted* document label.\n",
    "# The predicted document label is obtained by summing the feature vectors\n",
    "# for all sentences and classifying the resulting vector.\n",
    "def predict_lr_sent_labels_pred_doc(clf, documents, seed):\n",
    "    doc_preds = []\n",
    "    sent_preds = []\n",
    "    neu_label_idx = np.where(clf.classes_ == 'neu')[0]\n",
    "    for d in documents:\n",
    "        # sum sentence feature vectors and predict.\n",
    "        this_doc_pred_proba = clf.predict_proba(d.sentence_matrix.sum(axis=0))[0]\n",
    "        this_doc_pred_proba[neu_label_idx] = 0\n",
    "        this_doc_pred_idx = this_doc_pred_proba.argmax()\n",
    "        this_doc_pred = clf.classes_[this_doc_pred_idx]\n",
    "        probas = clf.predict_proba(d.sentence_matrix)\n",
    "        probas[:,neu_label_idx] = 0\n",
    "        preds = clf.predict(d.sentence_matrix)\n",
    "        doc_preds.append(d.sent_labels[probas[:, this_doc_pred_idx].argmax()])        \n",
    "        sent_preds.extend(preds)\n",
    "    return doc_preds, sent_preds\n",
    "\n",
    "clf = expt(documents, fit_lr_sent_labels, predict_lr_sent_labels_pred_doc)\n",
    "# print_top_feats(clf, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to ignoring document label, minimal improvement to accuracy, but helps neutrality a fair bit (6 docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|method|acc|neu|\n",
    "|------|---|---|\n",
    "|SaD|.962|.332|\n",
    "|Fine|.905|.301|\n",
    "|SaD+Oracle|**.969**|.332|\n",
    "|Fine+Oracle|.954|**.230**|\n",
    "|SaD+Pred|**.969**|.352|\n",
    "|Fine+Pred|.909|.270|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
